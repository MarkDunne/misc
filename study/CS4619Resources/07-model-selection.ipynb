{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:466e2fa43a96af0a5fc9abc8462080c4bc7e23fdfa2c468b0dc4106e8f3c8add"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model Selection"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    As discussed previously, the learning algorithm finds the values of parameters. But there are other parameters of\n",
      "    learning algorithms &mdash; ones that the learning algorithm does not set &mdash; which are called hyperparameters.\n",
      "    Our best example so far is the value of $k$ in kNN.\n",
      "</p>\n",
      "<p> \n",
      "    In this lecture, we look at ways of setting the values of the hyperparameters. This is called <b>model\n",
      "    selection</b>. Despite what it may sound like, this is not about choosing between different\n",
      "    learning algorithms (e.g. between OLS linear regression and kNN) &mdash; that, after all, can be done by\n",
      "    error estimation (which we discussed in a previous lecture). Model selection is about optimizing\n",
      "    the settings for each learning algorithm.\n",
      "</p>\n",
      "<p>\n",
      "    For some learning algorithms there might be special methods for setting some of the hyperparameters.\n",
      "    If not, in essence, you just try lots of values for your hyperparameters and see which is best &mdash; \n",
      "    an approach known as 'grid search'. But how do we estimate the error of each hyperparameter value that we try? \n",
      "    This is what we will discuss first.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Validation Sets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "   As in the holdout method, we randomly partition the dataset into disjoint sets. Here, these partitions are\n",
      "   called the training set and the <b>validation set</b>. \n",
      "</p>\n",
      "<p>\n",
      "    For each value of the hyperparameter that you wish to try, you train the learning algorithm on the training set. \n",
      "    So, assuming one hyperparameter with $\\mathit{numvals}$ values that you wish to try, you will learn $\\mathit{numvals}$ models. Let's designate\n",
      "    them by $h_1,\\ldots,h_{\\mathit{numvals}}$. You test each of them on\n",
      "    the validation set, e.g. measuring mean squared error, as before. From this, you can see which of the\n",
      "    $numvals$ models (corresponding to $numvals$ different values of the hyperparameter) appears to be the best &mdash; the\n",
      "    one with the lowest error on the validation set. This gives you the value of the hyperparameter. That's\n",
      "    model selection.\n",
      "</p>\n",
      "<p>\n",
      "    The weakness of this (as before) is that the training and validation sets may not be representative: we may get\n",
      "    'lucky' or 'unlucky'. So we need to use <em>resampling</em> again: either repeated holdout, $k$-fold cross-validation,\n",
      "    repeated $k$-fold cross-validation, leave-one-out cross-validation, or one of the other methods that we did not study. For\n",
      "    concreteness, let's focus on one of them: $k$-fold cross-validation.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$k$-Fold Cross-Validation for Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    To use $k$-fold cross-validation for model selection, you divide into $k$ folds. You take all but fold 1 as the\n",
      "    training set and you learn $\\mathit{numvals}$ models (one per hyperparameter value), and you test them on fold 1, recording the\n",
      "    mean squared error of each model. You repeat, taking a different fold to be the test set each time. For each\n",
      "    hyperparameter value, you take the mean of its $k$ mse's. The hyperparameter value with lowest mean mse is the value\n",
      "    that you should 'go live' with. (At this point, you can train on <em>all</em> the data, using the best\n",
      "    hyperparameter value that you have just discovered.) Here it is in pseudocode:\n",
      "</p>\n",
      "<ul style=\"background: lightgray; list-style: none\">\n",
      "    <li>\n",
      "        partition the dataset $D$ into $k$ disjoint equal-sized subsets, $T_1, T_2,\\ldots,T_k$\n",
      "    <li>\n",
      "    <li>\n",
      "        <b>for</b> $u = 1$ to $k$\n",
      "        <ul>\n",
      "            <li>\n",
      "                <b>for</b> $v = 1$ to $\\mathit{numvals}$\n",
      "                <ul>\n",
      "                    <li>train on $D \\setminus T_u$ with the $v$th hyperparameter value</li>\n",
      "                    <li>make predictions for $T_u$</li>\n",
      "                    <li>measure error (e.g. MSE)</li>\n",
      "                </ul>\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "    <li>\n",
      "        get the mean of the errors for each hyperparameter value\n",
      "    </li>\n",
      "    <li>\n",
      "        report the hyperparameter value with lowest mean mse\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Let's now combine this validation set idea with grid search &mdash; and let's do so\n",
      "    in scikit-learn.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Grid Search"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    <b>Grid search</b> is an 'exhaustive' approach &mdash; it tries every value for the hyperparameter (or, at least, all\n",
      "    values in a certain range). If there is more than one hyperparameter, it tries every combination of their values.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        If one hyperprameter has $\\mathit{numvals}_1$ values and another has $\\mathit{numvals}_2$ values, how many models will grid search train?\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Grid Search and Holdout in scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Let's try all values of kNN's hyperparameter $k$ between 1 and 10 for unweighted kNN on the Cork property dataset using\n",
      "    holdout (where we simply split the dataset once into training set and validation set):\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.cross_validation import ShuffleSplit\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['flarea', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values\n",
      "\n",
      "# Create kNN Regressor object\n",
      "estimator = KNeighborsRegressor()\n",
      "\n",
      "# Dictionary containing each hyperparameter and the hyperparameter#s possible values \n",
      "hyperparameters = {'n_neighbors' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
      "\n",
      "# We want to split the dataset just once into training and validation\n",
      "ss = ShuffleSplit(n = len(y), n_iter = 1, test_size = 0.3, random_state = np.random)\n",
      "\n",
      "# And here we create the Grid Search object and run its fit method\n",
      "gs = GridSearchCV(estimator, hyperparameters, scoring = 'mean_squared_error', cv = ss)\n",
      "gs.fit(X, y)\n",
      "\n",
      "# Display some of its instance variables afterwards\n",
      "print(gs.grid_scores_)\n",
      "print(gs.best_estimator_)\n",
      "print(gs.best_score_)\n",
      "print(gs.best_params_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[mean: -132211.82353, std: 0.00000, params: {'n_neighbors': 1}, mean: -169570.02206, std: 0.00000, params: {'n_neighbors': 2}, mean: -74210.66830, std: 0.00000, params: {'n_neighbors': 3}, mean: -60245.02757, std: 0.00000, params: {'n_neighbors': 4}, mean: -5904.72941, std: 0.00000, params: {'n_neighbors': 5}, mean: -17230.40564, std: 0.00000, params: {'n_neighbors': 6}, mean: -13653.57683, std: 0.00000, params: {'n_neighbors': 7}, mean: -14926.60823, std: 0.00000, params: {'n_neighbors': 8}, mean: -9240.86837, std: 0.00000, params: {'n_neighbors': 9}, mean: -14140.27809, std: 0.00000, params: {'n_neighbors': 10}]\n",
        "KNeighborsRegressor(algorithm=auto, leaf_size=30, metric=minkowski,\n",
        "          n_neighbors=5, p=2, weights=uniform)\n",
        "-5904.72941176\n",
        "{'n_neighbors': 5}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    This suggests a particular value for $k$. But, remember, we are doing only one split of the data. Re-run it and you \n",
      "    will see that the best value for the hyperparameter differs from run to run.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Grid Search and $k$-Fold Cross-Validation in scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    If we want to do $k$-fold cross-validation instead (which we hope will give a more robust answer), we simply feed a\n",
      "    different cross-validation method into the grid search. As we saw before, scikit-lkearn\n",
      "    makes it very easy to choose $k$-fold cross-validation: simply supply the number\n",
      "    of folds (but, remember, you may need ton do your own shuffling):\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "# And here's we create the Grid Search object and run its fit method\n",
      "gs = GridSearchCV(estimator, hyperparameters, scoring = 'mean_squared_error', cv = 10)\n",
      "gs.fit(X, y)\n",
      "\n",
      "# Display some of its instance variables afterwards\n",
      "print(gs.grid_scores_)\n",
      "print(gs.best_estimator_)\n",
      "print(gs.best_score_)\n",
      "print(gs.best_params_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[mean: -120836.04911, std: 229132.75725, params: {'n_neighbors': 1}, mean: -137319.21429, std: 242303.31122, params: {'n_neighbors': 2}, mean: -136624.22569, std: 241851.18113, params: {'n_neighbors': 3}, mean: -114827.41462, std: 228408.08430, params: {'n_neighbors': 4}, mean: -109692.46893, std: 233010.31668, params: {'n_neighbors': 5}, mean: -104796.04501, std: 242880.24167, params: {'n_neighbors': 6}, mean: -103158.51576, std: 255682.18331, params: {'n_neighbors': 7}, mean: -99171.72112, std: 256983.59966, params: {'n_neighbors': 8}, mean: -96955.54249, std: 258614.41602, params: {'n_neighbors': 9}, mean: -97213.61929, std: 263218.88941, params: {'n_neighbors': 10}]\n",
        "KNeighborsRegressor(algorithm=auto, leaf_size=30, metric=minkowski,\n",
        "          n_neighbors=9, p=2, weights=uniform)\n",
        "-96955.5424934\n",
        "{'n_neighbors': 9}\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    It selects $k = 9$ (although if we were feeding in a suitably-initialized <code>KFold</code>\n",
      "    object, you may still get different answers each time you run it, but with a lot less\n",
      "    variability than before).\n",
      "</p>\n",
      "<p>\n",
      "    Assuming we now want to 'go live', we would train 9NN on <em>all</em> the data.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Error Estimation, again"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Suppose you want to do error estimation, as before, i.e. you want to know how well a model\n",
      "    is likely to perform on unseen data. But suppose there is a hyperparameter and so you want \n",
      "    your estimate of the error on unseen data to use a good value for the hyperparameter. In this case, we need to combine\n",
      "    ideas from the earlier part of this lecture with ideas from the error estimation lecture.\n",
      "</p>\n",
      "<p>\n",
      "    The simplest case is to partition your data into three sets (assuming you have enough data!):\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        training set,\n",
      "    </li>\n",
      "    <li>\n",
      "        validation set, and\n",
      "    </li>\n",
      "    <li>\n",
      "        test set.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    You use the training set and the validation set to choose the best value for the hyperparameter, i.e. train $\\mathit{numvals}$\n",
      "    models (one for each value of the hyperparameter), test them on the validation set, using grid search.\n",
      "    Choose the value that gave\n",
      "    the lowest mse. Then, test the model you have just selected on the test set: the mse that this produces is your\n",
      "    estimate of the error on unseen examples.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Training Set, Validation Set and Test Set in scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In this scikit-learn example, we will split the data 60-20-20:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "# Split the data 80/20: the 80 is the combined training and validation sets; the 20 is the final test set\n",
      "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.2, random_state = np.random)\n",
      "\n",
      "# Split the combined set just once into training and validation: 75/25 gives 60/20 of the original data\n",
      "ss = ShuffleSplit(n = len(y_train_val), n_iter = 1, test_size = 0.25, random_state = np.random)\n",
      "\n",
      "# And here we create the Grid Search object and run its fit method\n",
      "gs = GridSearchCV(estimator, hyperparameters, scoring = 'mean_squared_error', cv = ss)\n",
      "gs.fit(X, y)\n",
      "\n",
      "# Take the best estimator found by the grid search\n",
      "# Test it on the test set \n",
      "y_predicted = gs.predict(X_test)\n",
      "mse_test = mean_squared_error(y_test, y_predicted)\n",
      "\n",
      "# Display\n",
      "mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "51752.965277777781"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    This estimate has the weakness (again) that it is based on just one way of partitioning the data: different partitions\n",
      "    will have very different error estimates &mdash; as you can confirm by running the code seveal times.\n",
      "    We need to use resampling.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Nested $k$-Fold Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    What we will do is called <b>nested $k$-fold cross validation</b>. As usual, it partitions the dataset into $k$ folds.\n",
      "    One fold is kept as the test set. The remaining data is then partitioned again into $k$ folds. One of <em>these</em>\n",
      "    folds is kept as the validation set. The remaining is used for training. We learn $\\mathit{numvals}$ models from the training data\n",
      "    (one for each value of the hyperparameter). We test each of them on the validation set. We repeat this, taking each\n",
      "    <em>inner fold</em> in turn as the validation set. When we have done, we can select the best value of the hyperparameter.\n",
      "    We test this model on the <em>outer fold</em> that we kept as test set. Then the whole thing repeats: a different outer\n",
      "    fold becomes the test set; $k$-fold cross-validation is run on the remaining data to find another best value for\n",
      "    the hyperparameter; and this model is tested on the outer fold that was kept as test set. And so on.\n",
      "</p>\n",
      "<p>\n",
      "    In pseudocode:\n",
      "</p>\n",
      "<ul style=\"background: lightgray; list-style: none\">\n",
      "    <li>\n",
      "        partition the dataset $D$ into $k$ disjoint equal-sized subsets, $T_1, T_2,\\ldots,T_k$\n",
      "    <li>\n",
      "    <li>\n",
      "        <b>for</b> $u_1 = 1$ to $k$\n",
      "        <ul>\n",
      "            <li>\n",
      "                let $D'$ be $D \\setminus T_{u_1}$\n",
      "            </li>\n",
      "            <li>\n",
      "                partition $D'$ into $k$ disjoint equal-sized subsets, $S_1, S_2,\\ldots,S_k$\n",
      "            </li>\n",
      "            <li>\n",
      "                <b>for</b> $u_2 = 1$ to $k$\n",
      "                <ul>\n",
      "                    <li>\n",
      "                        <b>for</b> $v = 1$ to $\\mathit{numvals}$\n",
      "                        <ul>\n",
      "                            <li>train on $D' \\setminus S_{u_2}$ with the $v$th hyperparameter value</li>\n",
      "                            <li>make predictions for $S_{u_2}$</li>\n",
      "                            <li>measure validation error (e.g. MSE)</li>\n",
      "                        </ul>\n",
      "                    </li>\n",
      "                </ul>\n",
      "            </li>\n",
      "            <li>\n",
      "                get the mean of the errors for each hyperparameter value\n",
      "            </li>\n",
      "            <li>\n",
      "                select the model (hyperparameter value) with lowest mean mse\n",
      "            </li>\n",
      "            <li>\n",
      "                use the selected model to make predictions for $T_{u_1}$\n",
      "            </li>\n",
      "            <li>\n",
      "                measure test error (e.g. MSE)\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "    <li>\n",
      "        report the means of the test errors\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Nested $k$-Fold Cross-Validation in scikit-learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "outer_kf = KFold(n = len(y), n_folds = 10, shuffle = True, random_state = np.random)\n",
      "inner_kf = KFold(n = int(len(y) * 9 / 10.0), n_folds = 10, shuffle = True, random_state = np.random)\n",
      "gs = GridSearchCV(estimator, hyperparameters, scoring = 'mean_squared_error', cv = inner_kf)\n",
      "mses_test = np.abs(cross_val_score(gs, X, y, scoring = 'mean_squared_error', cv = outer_kf))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "152073.62807795344"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Happily, there's a shorter version again (but again probably requiring you to do the shuffling):\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(estimator, hyperparameters, scoring = 'mean_squared_error', cv = 10)\n",
      "mses_test = np.abs(cross_val_score(gs, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "101001.56507719708"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Of course, our little dataset really isn't big enough for this kind of treatment.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Confusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    You might be thinking to yourself: \"But the best value for the hyperparameter can be different for each outer fold.\" You are\n",
      "    correct. And so then you may be thinking: \"So which value for the hyperparameter do I choose?\" To which the reply is: \n",
      "    \"None of them. This is not model selection. This is just error estimation.\" If you want to do model selection, see\n",
      "    earlier.\n",
      "</p>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}