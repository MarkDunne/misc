{
 "metadata": {
  "name": "",
  "signature": "sha256:f0320039b16f1bc26e631207e353b56a9ffceb4bc54b218a15c05733129447ab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Error Estimation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Mean Squared Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    So, you've trained an estimator on a training set. \n",
      "    You want to know how well it will do in practice, once you start to use it to make predictions.\n",
      "    Easy right? We have the training set, so we measure how well the estimator peforms on the training set.\n",
      "    For each example in the training set, we ask the estimator to predict the value of the dependent variable\n",
      "    and compare with the <em>actual</em> value, which is also in the training set.\n",
      "</p>\n",
      "<p>\n",
      "    For regression, we will compute the <b>mean squared error</b>:\n",
      "    $$\\frac{1}{m}\\sum_{i=0}^m(\\hat{y_i} - y_i)^2$$\n",
      "    where $\\hat{y_i}$ is the predicted value for example $i$ and $y_i$ is the actual value.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example of Mean Squared Error on the Training Set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Let's compute the mean squared error for Linear Regression trained on the Cork property dataset: \n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['flarea', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values\n",
      "\n",
      "# Create linear regression object\n",
      "estimator = LinearRegression()\n",
      "\n",
      "# Train the model using the data\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Use the learned model to predict on the same examples\n",
      "y_predicted = estimator.predict(X)\n",
      "\n",
      "# Compute the mean squared error\n",
      "mse = mean_squared_error(y, y_predicted)\n",
      "\n",
      "# Display\n",
      "mse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "55456.55323798101"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Training Error and Test Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We will refer to the error on the training set as the <b>training error</b>. (Some people call it the\n",
      "    'resubstitution error' and sometimes the 'in-sample error'.) But, remember, we're not much\n",
      "    interested in how well we have done on this data; we want to know how well we will perform in the future, \n",
      "    on unseen data. Is the training error a good indicator of performance on unseen data? The answer is, \n",
      "    in general: no.\n",
      "</p>\n",
      "<p>\n",
      "    The estimator's training error (its performance on the very data on which it was trained) is likely to\n",
      "    give an optimistic, even very optimistic, view of its future performance.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        One intuition of why this is wrong is that it's a bit like\n",
      "        a teacher who sets exams whose questions test the very same examples s/he used when teaching the\n",
      "        material.\n",
      "    </li>\n",
      "    <li>\n",
      "        Another intuition of why this is wrong: which estimator that we have studied can have\n",
      "        zero training error but would be likely to perform much less well in practice?\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    (By the way, although the training error is not a good predictor of future performance, it can \n",
      "    still be useful, as we will see in the lecture on Underfitting and Overfitting.)\n",
      "</p>\n",
      "<p>\n",
      "    To predict future performance, we need to measure error on an <em>independent</em> dataset &mdash; one\n",
      "    that played no part in creating the estimator. This second dataset is called the <b>test set</b>, and\n",
      "    our error on the test set we will call the <b>test error</b>. (In some circumstances people might call it \n",
      "    the 'out-of-sample error' or 'extra-sample error'.)\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        If you have a ready supply of quality data, then collect one very large dataset \n",
      "        to be the training\n",
      "        set, and collect another very large dataset to be the test set. But large datasets are not\n",
      "        always available, and large high-quality datasets are even harder to come by. (And, remember, it must also \n",
      "        have the actual values of the dependent variable as well as the values of the features.)\n",
      "    </li>\n",
      "    <li>\n",
      "        If the supply of data is more limited, then collect one dataset (as large as you can) and partition it \n",
      "        into training set and test set. \n",
      "        This is called the <b>holdout</b> method, because the test set\n",
      "        is withheld during training. It is essential that the test set is not used in any way to create\n",
      "        the estimator. We look at holdout, and variations of it, in more detail in this lecture.\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Holdout"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "   We split the dataset randomly but ensuring the two sets are <em>disjoint</em>.\n",
      "   There is a tension here. To learn a good estimator, we want the training set to be as big as possible.\n",
      "   But for a good prediction of future performance, we want the test set to be as big as possible.\n",
      "   Commonly, the training set will be between 50% and 80% of the dataset.\n",
      "</p>\n",
      "<p>\n",
      "    Splitting the dataset in this way is very easy in scikit-learn:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = np.random)\n",
      "\n",
      "# Create linear regression object\n",
      "estimator = LinearRegression()\n",
      "\n",
      "# Train the model using the training set\n",
      "estimator.fit(X_train, y_train)\n",
      "\n",
      "# The training error\n",
      "# Predict on the training set and measure the difference between the predictions and the actual values in the training set:\n",
      "y_predicted = estimator.predict(X_train)\n",
      "mse_train = mean_squared_error(y_train, y_predicted)\n",
      "\n",
      "# The test error\n",
      "# Predict on the test set and measure the difference between the predictions and the actual values in the test set:\n",
      "y_predicted = estimator.predict(X_test)\n",
      "mse_test = mean_squared_error(y_test, y_predicted)\n",
      "\n",
      "# Display\n",
      "mse_train, mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "(72457.840398695451, 18432.326882260084)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    You will find it instructive to run the above again and again to see the effect of different random splits.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Pros and Cons of Holdout"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The advantage of this method is that the test error is independent of the training set.\n",
      "</p>\n",
      "<p>\n",
      "    The disadvantages of the holdout method are:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        You will observe that results can vary quite a lot. Informally, you might get lucky &mdash; or unlucky.\n",
      "        Maybe you get a very 'helpful' training set, or a very 'unhelpful' training set; a very 'easy-to-predict' \n",
      "        test set, or a very 'hard-to-predict' test set. In other words,\n",
      "        in any one split, the data used for training or testing might not be representative.\n",
      "    </li>\n",
      "    <li>\n",
      "        We are training on only a subset of the available dataset, perhaps as little as 50% of it. From so little\n",
      "        data, we may learn a worse model and so our error measurement may be pessimistic.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    In practice, you would not use the holdout method &mdash; unless you had a very large dataset that\n",
      "    would mitigate the above problems. Instead, you would use one of its variants that we \n",
      "    describe below. Each of these variants uses <b>resampling</b>, meaning that the examples get re-used\n",
      "    for training and testing.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Repeated Holdout"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    One solution to the problem of biased holdout sets is to <em>repeat</em> the whole process:\n",
      "</p>\n",
      "<ul style=\"background: lightgray; list-style: none\">\n",
      "    <li>\n",
      "        repeatedly\n",
      "        <ul>\n",
      "            <li>split the dataset into training and test sets</li>\n",
      "            <li>train on the training set</li>\n",
      "            <li>make predictions for the test set</li>\n",
      "            <li>measure error (e.g. MSE)</li>\n",
      "        </ul>\n",
      "        report the mean of the errors\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Illustrating scikit-learn's ShuffleSplit Class"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    scikit-learn provides a ShuffleSplit class, which is an \n",
      "    iterator that gives Boolean indexes that split the dataset. Here's a simple use:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import ShuffleSplit\n",
      "\n",
      "# The dataset has 10 elements. Split it 70%/30%. Do so 3 times. Return an iterator over the indexes\n",
      "ss = ShuffleSplit(n = 10, n_iter = 3, test_size = 0.3, random_state = np.random)\n",
      "\n",
      "# Display the indexes\n",
      "for train_indexes, test_indexes in ss:\n",
      "    print train_indexes, test_indexes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[8 4 0 6 9 1 5] [7 2 3]\n",
        "[2 6 1 0 4 3 9] [8 5 7]\n",
        "[1 4 2 5 9 0 6] [3 7 8]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Using ShuffleSplit to Compute Training Error and Test Error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import ShuffleSplit\n",
      "\n",
      "def repeated_holdout_for_regression(estimator, X, y, num_iterations = 10, test_size = 0.3):\n",
      "    mses_train = np.zeros(num_iterations)\n",
      "    mses_test = np.zeros(num_iterations)\n",
      "    ss = ShuffleSplit(n = len(y), n_iter = num_iterations, test_size = test_size, \n",
      "                      random_state = np.random)\n",
      "    for i, (train_indexes, test_indexes) in zip(range(num_iterations), ss):\n",
      "        X_train = X[train_indexes]\n",
      "        y_train = y[train_indexes]\n",
      "        X_test = X[test_indexes]\n",
      "        y_test = y[test_indexes]\n",
      "        estimator.fit(X_train, y_train)\n",
      "        y_predicted = estimator.predict(X_train)\n",
      "        mses_train[i] = mean_squared_error(y_train, y_predicted)\n",
      "        y_predicted = estimator.predict(X_test)\n",
      "        mses_test[i] = mean_squared_error(y_test, y_predicted)\n",
      "    return np.mean(mses_train), np.mean(mses_test)\n",
      "\n",
      "# Here's an example of calling the function:\n",
      "estimator = LinearRegression()\n",
      "mean_mse_train, mean_mse_test = repeated_holdout_for_regression(estimator, X, y)\n",
      "mean_mse_train, mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "(51732.57780736428, 80806.160479259182)"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Using ShuffleSplit to Compute Test Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    scikit-learn does provide a more convenient way of doing this, but it only computes the test error and it negates the\n",
      "    values so that 'higher is better':\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import ShuffleSplit\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "ss = ShuffleSplit(n = len(y), n_iter = 10, test_size = 0.3, random_state = np.random)\n",
      "estimator = LinearRegression()\n",
      "mses_test = np.abs(cross_val_score(estimator, X, y, scoring = 'mean_squared_error', cv = ss))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "59578.364218363036"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Pros and Cons of Repeated Holdout"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The advantage here is we can repeat indefinitely to improve our confidence. The disadvantage is training\n",
      "    sets may overlap with each other and test sets may overlap with each other, although the effect of this\n",
      "    is reduced if the dataset is large.\n",
      "</p>\n",
      "<p>\n",
      "    Let's look at another method.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$k$-Fold Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In this approach, we randomly partition the data into $k$ disjoint subsets of equal size. (This is a different use of \n",
      "    $k$ from the $k$ in kNN.) Each of the partitions is called a <b>fold</b>. Typically, $k = 10$, so you have 10 folds. \n",
      "    But, for conventional statistical significance testing to be applicable, you should probably ensure that the number of\n",
      "    examples in each fold does not fall below 30. If this isn't possible, then either use a smaller value for $k$, or\n",
      "    do not use $k$-fold cross validation. \n",
      "</p>\n",
      "<p>\n",
      "    You take each fold in turn and use it as the test set, training the learner on \n",
      "    the remaining folds. Clearly, you can do this $k$ times, so that each fold gets 'a turn' at being the test set.\n",
      "</p>\n",
      "<ul style=\"background: lightgray; list-style: none\">\n",
      "    <li>\n",
      "        partition the dataset $D$ into $k$ disjoint equal-sized subsets, $T_1, T_2,\\ldots,T_k$\n",
      "    <li>\n",
      "    <li>\n",
      "        <b>for</b> $i = 1$ to $k$\n",
      "        <ul>\n",
      "            <li>train on $D \\setminus T_i$</li>\n",
      "            <li>make predictions for $T_i$</li>\n",
      "            <li>measure error (e.g. MSE)</li>\n",
      "        </ul>\n",
      "        report the mean of the errors\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    By this method, each example is used exactly once for testing, and $k - 1$ times for training.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Pros and Cons of $k$-Fold Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Compared with repeated holdout, the advantages of this method are:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        The test errors of the folds are independent &mdash; because examples are included in only one test set. \n",
      "    </li>\n",
      "    <li>\n",
      "        Better use is made of the dataset: for $k = 10$, for example, we train using 9/10 of the dataset.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "     The disadvantages are: \n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        While the test sets are independent of each other, the training sets are not: they will overlap\n",
      "        with each other to some degree. (This effect of this will be less, of course, for larger datasets.)\n",
      "    </li>\n",
      "    <li>\n",
      "        The number of folds is constrained by the size of the dataset and the desire to have folds of\n",
      "        at least 30 examples.\n",
      "    </li>\n",
      "    <li>\n",
      "        It can be costly to train the learning algorithm $k$ times.\n",
      "    </li>\n",
      "    <li>\n",
      "        There may still be some variability in the results due to 'lucky'/'unlucky' splits &mdash; which\n",
      "        motivates Repeated $k$-Fold Cross-Validation, below.\n",
      "    </li>\n",
      "</ul>\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Illustrating scikit-learn's KFold Class"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    scikit-learn provides the KFold class, which is an iterator, similar to the ShuffleSplit class. Here's a simple use:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "# The dataset has 10 elements and we want 5 folds\n",
      "kf = KFold(n = 10, n_folds = 5, shuffle = True, random_state = np.random)\n",
      "\n",
      "# Display the indexes\n",
      "for train_indexes, test_indexes in kf:\n",
      "    print train_indexes, test_indexes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 2 3 5 7 8 9] [4 6]\n",
        "[1 2 3 4 5 6 7 8] [0 9]\n",
        "[0 2 4 5 6 7 8 9] [1 3]\n",
        "[0 1 2 3 4 5 6 9] [7 8]\n",
        "[0 1 3 4 6 7 8 9] [2 5]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Using KFold to Compute Test Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Assuming that we are happy to get just the test error, we can use the cross_val_score method again:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "kf = KFold(n=len(y), n_folds=10, shuffle = True, random_state = np.random)\n",
      "estimator = LinearRegression()\n",
      "mses_test = np.abs(cross_val_score(estimator, X, y, scoring = 'mean_squared_error', cv = kf))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "68955.237792380722"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    But, $k$-fold cross-validation is so commonplace, that there is a shorter way to write\n",
      "    the code above, as follows:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = LinearRegression()\n",
      "mses_test = np.abs(cross_val_score(estimator, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "100047.69646341192"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Be warned, however, this almost certainly does not shuffle the dataset before splitting it into folds.\n",
      "    Q: Why might that be a problem?\n",
      "</p>\n",
      "<p>\n",
      "    You should probably shuffle the <code>DataFrame</code> just after reading it in from the CSV file using, e.g.:<br>\n",
      "    <code>df = df.take(np.random.permutation(len(df)))</code>\n",
      "</p>\n",
      "<p>\n",
      "    Final observation: In the above, we ran the 10-fold cross validation on the Cork property dataset. That dataset has \n",
      "    only 224 examples\n",
      "    &mdash; not enough examples to give at least 30 examples in each of the 10 folds. So this isn't an ideal use of \n",
      "    the method.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Repeated $k$-Fold Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    It's not uncommon to find people repeating the $k$-fold cross validation to reduce variability in the results. \n",
      "    For example, you might run 10 times 10-fold\n",
      "    cross-validation and average the results. This means running the learning algorithm 100 times, each time on a training\n",
      "    set that is nine tenths of the full dataset &mdash; quite computationally expensive.\n",
      "</p>\n",
      "<p>\n",
      "    We won't look at the code. Straightforwardly, you wrap an extra loop around the code we gave above. \n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Leave-One-Out Cross-Validation (LOOCV)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Leave-one-out cross-validation is $k$-fold cross-validation in which $k = m$, the number of examples in the dataset:\n",
      "    each example is in its own fold. In other words, you train the learner on all examples but one, and that one remaining\n",
      "    example is used for testing. And you do this in turn for each example in the dataset. You'll get $m$ error values, which you\n",
      "    can average.\n",
      "</p>\n",
      "<ul style=\"background: lightgray; list-style: none\">\n",
      "    <li>\n",
      "        <b>for</b> $i = 1$ to $m$\n",
      "        <ul>\n",
      "            <li>train on $D \\setminus \\Set{\\v{x}^{(i)}}$</li>\n",
      "            <li>make prediction for $\\v{x}^{(i)}$</li>\n",
      "            <li>measure error (e.g. MSE)</li>\n",
      "        </ul>\n",
      "        report the mean of the errors\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    As with $k$-fold cross-validation, each example is used exactly once in a test set. But each example is used in $m - 1$ \n",
      "    different training sets. \n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Pros and Cons of LOOCV"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    There are advantages:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        One advantage of LOOCV is that the maximum amount of data is used for training, which makes an accurate\n",
      "        estimator more likely. (But, see the disadvantage below.)\n",
      "    </li>\n",
      "    <li>\n",
      "        Another advantage is that there is no randomness: we can't get lukcy or unlucky. And there's \n",
      "        no point in repeating the process, we'll get the same result each time.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    But there are disadvantages:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        The obvious disadvantage is the cost: the learner must be trained $m$ times, and each time it will be trained on almost \n",
      "        all the data. This method is therefore infeasible in some cases. \n",
      "        <p>\n",
      "            Question:\n",
      "        </p>\n",
      "        <ul>\n",
      "            <li>\n",
      "                For which estimator do you think LOOCV is fairly common? Why?\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "    <li>\n",
      "        More subtly, LOOCV's $m$ models are trained on almost identical data; in $k$-Fold Cross-Validation, the $k$ models\n",
      "        are trained on data with less overlap.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    (Advanced note, which you can ignore: We said that LOOCV must train the learning algorithm $m$ times. In fact, \n",
      "    for some learners, including OLS linear regression, you can learn just the first model and then, with a bit of \n",
      "    maths, work out the final average error without learning any of the other models. So this makes LOOCV practical \n",
      "    for this class of learning algorithms.)\n",
      "</p>\n",
      "<p>\n",
      "    You can see that there is a trade-off here. Empirically, $k$-Fold Cross-Validation with $k = 5$ or $k = 10$ tends\n",
      "    to report the most reliable error figures.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Using LeaveOneOut to Compute Test Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Here is LOOCV in scikit-learn:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import LeaveOneOut\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "loocv = LeaveOneOut(n = len(y))\n",
      "estimator = LinearRegression()\n",
      "mses_test = np.abs(cross_val_score(estimator, X, y, scoring = 'mean_squared_error', cv = loocv))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "72376.115691746352"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Final Remarks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ol>\n",
      "    <li>\n",
      "        There are methods other than those covered including Bootstrapping and Permutation Tests.\n",
      "    </li>\n",
      "    <li>\n",
      "        So you've used one of the above methods and found the test error of your estimator. The dirty secret\n",
      "        of Machine Learning is this: at this point, if dissatisfied with the test error, many Machine\n",
      "        Learning researchers, start tweaking their learning algorithms to try to bring down the test error.\n",
      "        This is wrong! It is called <b>leakage</b>: knowledge of the test set is being used to develop the\n",
      "        estimator. It's like the teacher letting the students take the same exam again. It will result in\n",
      "        the test error giving an optimistic view of the ultimate performance of the estimator on unseen data.\n",
      "        <p>\n",
      "            If you must do something like this, then somewhat less problematic is if you ensure that you are \n",
      "            using different random splits when evaluating your tweaks.\n",
      "        </p>\n",
      "    </li>\n",
      "    <li>\n",
      "        Finally, suppose you have used one of the above methods to estimate the error of your regressor. \n",
      "        You are ready to release your regressor on the world. At this point, you can train it on\n",
      "        <em>all</em> the examples in your dataset, so as to maximize the use of the data.\n",
      "    </li>\n",
      "</ol>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}