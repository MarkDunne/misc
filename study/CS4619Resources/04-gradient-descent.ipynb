{
 "metadata": {
  "name": "",
  "signature": "sha256:0c7da28e4bfcbc842aadcfae1d7284dce3bc4604fdcb377f8d7573c88259e9f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "OLS Linear Regression using Gradient Descent"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "OLS Regression using Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    A reminder: we have a training set containing $m$ examples, each having $n$ features. We have \n",
      "    represented the training set as an $m \\times(n+1)$ matrix $\\v{X}$: each row is one of the examples;\n",
      "    each column is one of the features; but all the values in the first column (which we designate column 0)\n",
      "    are set to 1. Each hypothesis is a linear equation, $h_{\\v{\\beta}}(\\v{x}) = \\v{\\beta}_0\\v{x}_0 +\n",
      "    \\v{\\beta}_1\\v{x}_1 + \\ldots + \\v{\\beta}_n\\v{x}_n$. We are trying to find a \n",
      "    $(n+1)$-dimensional vector $\\v{\\beta}$ of parameters that minimise\n",
      "    the loss function $J(\\v{\\beta}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)})^2$. \n",
      "</p>\n",
      "<p>\n",
      "    In the previous lecture, we saw that one approach is to set the gradient of the loss function to zero \n",
      "    and to solve for $\\v{\\beta}$, giving what is called the normal equation. In this lecture, we take a \n",
      "    different approach: we <em>search</em> for the $\\v{\\beta}$ that minimises the loss function.\n",
      "</p>\n",
      "<p>\n",
      "    Conceptually, this approach works as follows:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "         It starts with an initial guess &mdash; e.g. choose each $\\v{\\beta}_j$ randomly or set each to zero\n",
      "    </li>\n",
      "    <li>\n",
      "        It computes $J(\\v{\\beta})$ &mdash; the total error for this hypothesis on the training set\n",
      "    </li>\n",
      "    <li>\n",
      "        And it repeats: it chooses another $\\v{\\beta}$ &mdash; one for which $J(\\v{\\beta})$ is smaller\n",
      "    </li>\n",
      "    <li>\n",
      "        It keeps doing this until $J(\\v{\\beta})$ <b>converges</b> &mdash; changes to $\\v{\\beta}$ do not result\n",
      "        in smaller $J(\\v{\\beta})$\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    So the key to this algorithm is how it comes up with new parameter values, $\\v{\\beta}$.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    <b>Gradient descent</b> is a <em>general</em> method for finding parameters $\\v{\\beta}$ that minimize some\n",
      "    function $J_{\\v{\\beta}}$. The idea is to repeatedly make small changes to the values of the parameters that\n",
      "    lead to the greatest immediate decrease in the value of $J(\\v{\\beta})$. The algorithm is as follows: \n",
      "</p>\n",
      "<ul style=\"background: lightgrey; list-style: none\">\n",
      "    <li>\n",
      "        repeat\n",
      "        <ul>\n",
      "            <li>\n",
      "                <em>simultaneously</em> update all $\\v{\\beta}_j$ as follows:<br />\n",
      "                $\\v{\\beta}_j \\gets \\v{\\beta}_j - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_j}$\n",
      "            </li>\n",
      "        </ul>\n",
      "        until convergence\n",
      "     </li>\n",
      "</ul>\n",
      "<p>\n",
      "    $\\alpha$ is called the <b>learning rate</b> and it controls the size of the changes that we make.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Importance of Simultaneous Update"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    It's important that the updates to the $\\v{\\beta}_j$ are done simultaneously, not one after the other. \n",
      "    We will see how to achieve this using vectorization. The alternative, if you want to stick with a more\n",
      "    conventional programming style, is to use the following statements:\n",
      "</p>\n",
      "<ul style=\"background: lightgrey\">\n",
      "    <li>\n",
      "        $\\v{\\delta}_0 \\gets \\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_0}$<br />\n",
      "        $\\v{\\delta}_1 \\gets \\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_1}$<br />\n",
      "        $\\vdots$<br />\n",
      "        $\\v{\\delta}_n \\gets \\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_n}$<br />\n",
      "        <br />\n",
      "        $\\v{\\beta}_0 \\gets \\v{\\beta}_0 - \\alpha\\v{\\delta}_0$<br />\n",
      "        $\\v{\\beta}_1 \\gets \\v{\\beta}_1 - \\alpha\\v{\\delta}_1$<br />\n",
      "        $\\vdots$<br />\n",
      "        $\\v{\\beta}_n \\gets \\v{\\beta}_n - \\alpha\\v{\\delta}_n$<br />\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    A student reads the above, takes no notice, and simply implements the algorithm with the following \n",
      "    <em>sequence</em> of statements:\n",
      "</p>\n",
      "<ul style=\"background: lightgrey\">\n",
      "    <li>\n",
      "        $\\v{\\beta}_0 \\gets \\v{\\beta}_0 - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_0}$<br />\n",
      "        $\\v{\\beta}_1 \\gets \\v{\\beta}_1 - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_1}$<br />\n",
      "        $\\vdots$<br />\n",
      "        $\\v{\\beta}_n \\gets \\v{\\beta}_n - \\alpha\\frac{\\partial J(\\v{\\beta})}{\\partial\\v{\\beta}_n}$<br />\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    What's the difference?\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Further Questions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Some further questions that we will discuss in the lecture:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        Why do these update rules, using derivatives, make intuitive sense. (We will explain this with\n",
      "        a diagram in which we plot $J_{\\beta}$ on the vertical axis and just one of the parameters on\n",
      "        the horizontal axis. We will consider separately the case where the gradient is positive and where\n",
      "        it is negative and we will see how the update rules move us towards a minimum.)\n",
      "    </li>\n",
      "    <li>\n",
      "        In practice, how will we define convergence?\n",
      "    </li>\n",
      "    <li>\n",
      "        What is the role of the learning rate, $\\alpha$?\n",
      "        <ul>\n",
      "            <li>\n",
      "                What if the value of $\\alpha$ is 'too small'?\n",
      "            </li>\n",
      "            <li>\n",
      "                What if the value of $\\alpha$ is 'too large'?\n",
      "            </li>\n",
      "            <li>\n",
      "                Some people suggest a variant of the algorithm in which the value of $\\alpha$ is decreased\n",
      "                over time, i.e. its value in later iterations is smaller. Why do they suggest this? And why\n",
      "                isn't it necessary?\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "    <li>\n",
      "        What happens if $J_{\\v{\\beta}}$ isn't convex?\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Gradient Descent for OLS Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    All that we need to apply this general algorithm to our specific scenario is to plug in the partial\n",
      "    derivative of our definition of $J_{\\beta}$. We want this:\n",
      "    $$\\frac{\\partial\\frac{1}{2m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)})^2}{\\partial\\v{\\beta}_j}$$\n",
      "    which is\n",
      "    $$\\frac{1}{m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$$\n",
      "</p>\n",
      "<p>\n",
      "    So we now get gradient descent for OLS linear regression:\n",
      "</p>\n",
      "<ul style=\"background: lightgrey; list-style: none\">\n",
      "    <li>\n",
      "        repeat\n",
      "        <ul>\n",
      "            <li>\n",
      "                <em>simultaneously</em> update $\\v{\\beta}_j$  as follows:<br />\n",
      "                $\\v{\\beta}_j \\gets \\v{\\beta}_j - \n",
      "                    \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$\n",
      "            </li>\n",
      "        </ul>\n",
      "        until convergence\n",
      "     </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Gradient Descent for OLS Regression in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Watch out for the vectorized implementation of the simultaneous update rules!\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def J(beta, X, y):\n",
      "    \"\"\"\n",
      "    Loss function for OLS regression\n",
      "    \"\"\"\n",
      "    h = X.dot(beta)\n",
      "    differences = h.flatten() - y\n",
      "    sq_differences = differences ** 2\n",
      "    return (1.0 / (2 * y.size)) * sq_differences.sum()\n",
      "\n",
      "def gds_for_ols_linear_regression(X, y, alpha, num_iterations):\n",
      "    \"\"\"\n",
      "    Gradient descent search for OLS linear regression.\n",
      "    alpha is the learning rate.\n",
      "    num_iterations is the numer of updates - instead of a better definition of convergence.\n",
      "    It returns parameters beta and also a numpy array of size num_iterations, containing\n",
      "    the value of the loss function, J, after each iteration - so you can plot it.\n",
      "    \"\"\"\n",
      "\n",
      "    Jvals = np.zeros(num_iterations)\n",
      "    m, n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    \n",
      "    for iter in range(num_iterations):\n",
      "        beta -= (1.0 * alpha / m) * (X.dot(beta) - y).dot(X)\n",
      "        Jvals[iter] = J(beta, X, y)\n",
      " \n",
      "    return beta, Jvals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Running the Code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We'll run it on the Cork property dataset but, for reasons that we will discuss in a later lecture, I'm\n",
      "    going to leave out the floor area feature.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# Insert the extra feature (all ones)\n",
      "df.insert(loc=0, column='ones', value=1)\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['ones', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values\n",
      "\n",
      "# Run the GDS\n",
      "beta, Jvals = gds_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 4000)\n",
      "\n",
      "# Display beta\n",
      "beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([-57.66816632,  49.06247762,  91.69181718])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    So, using just those two features ($\\v{x}_2$ and $\\v{x}_3$, the number of bedrooms and bathrooms resp.)\n",
      "    and a learning rate of 0.03, after 4000 iterations, the model we have learned is\n",
      "    $$y = -57.67 + 49.06\\v{x_2} + 91.69\\v{x_3}$$\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Sanity Check"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    It's a good idea to plot the values of the loss function against the number of iterations. If its value\n",
      "    ever increases, then\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        the code might be incorrect (I think it's OK!)\n",
      "    </li>\n",
      "    <li>\n",
      "        the value of $\\alpha$ is too big and is causing divergence\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    So let's do that:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(8,6))\n",
      "plt.title(\"$J$ during learning\")\n",
      "plt.xlabel(\"Number of iterations\")\n",
      "plt.xlim(1, Jvals.size)\n",
      "plt.ylabel(\"$J$\")\n",
      "plt.scatter(np.linspace(1, Jvals.size, Jvals.size), Jvals)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAGLCAYAAACr28ryAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkSZIkSRrZ/wdFi+OhoaZS4QAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fa387027a10>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stochastic Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The algorithm we have presented is sometimes called <b>batch gradient descent</b>. On every iteration,\n",
      "    for every parameter, it computes the loss function over <em>all</em> the examples in the training set.\n",
      "    Even with a highly optimized vectorized implementation, this may not scale well to very, very large \n",
      "    training sets. If there are hundreds of millions of examples, say, then they may not even fit in main\n",
      "    memory, in which case they have to be repeatedly read in from disk.\n",
      "</p>\n",
      "<p>\n",
      "    An alternative is <b>stochastic gradient descent</b> (or 'incremental gradient descent'). It looks at\n",
      "    each example in turn, and modifies the parameters $\\v{\\beta}$ on the basis of that individual example.\n",
      "    Here's the pseudocode:\n",
      "</p>\n",
      "<ul style=\"background: lightgrey; list-style: none\">\n",
      "    <li>\n",
      "        repeat\n",
      "        <ul>\n",
      "            <li>for $i \\gets 1$ to $m$\n",
      "                <ul>\n",
      "                    <li>\n",
      "                        <em>simultaneously</em> update $\\v{\\beta}_j$  as follows:<br />\n",
      "                        $\\v{\\beta}_j \\gets \\v{\\beta}_j - \n",
      "                        \\alpha(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)}) \\times \\v{x}_j^{(i)}$\n",
      "                    </li>\n",
      "                </ul>\n",
      "            </li>\n",
      "        </ul>\n",
      "        until convergence\n",
      "     </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Because it is not guided by the <em>global</em> minimum, some of the individual parameter updates taken in\n",
      "    stochastic gradient descent may not reduce the loss function: its 'journey' towards convergence may be\n",
      "    less direct. On the other hand, if you use it only when you have massive training sets, stochastic\n",
      "    gradient descent might actually take fewer iterations (of the outermost loop) than you require for\n",
      "    typical uses of batch gradient descent.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "OLS Linear Regression: Normal Equation vs Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Gradient Descent is such an important algorithm in machine learning and AI in general, that it was worth\n",
      "    studying it. Numerous machine learning algorithms make use of it.\n",
      "</p>\n",
      "<p>\n",
      "    But, for OLS linear regression, you probably wouldn't use it. The normal equation gives a much more direct\n",
      "    method. Here's a comparison of the two:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        Gradient Descent requires that we choose a learning rate, $\\alpha$; \n",
      "        the normal equation does not.\n",
      "    </li>\n",
      "    <li>\n",
      "        Gradient Descent requires a definition of convergence; the normal equation does not.\n",
      "    </li>\n",
      "    <li>\n",
      "        Gradient Descent can have problems with convergence when different features have very different\n",
      "        ranges of values: see the discussion of feature scaling in an upcoming lecture. These problems do not \n",
      "        arise with the normal equation.\n",
      "    </li>\n",
      "    <li>\n",
      "        But Gradient Descent can handle large numbers of features; you might prefer it if the number of \n",
      "        features $n$ is very large (several thousand, say). The normal equation does not scale so well to\n",
      "        very large numbers of features: $(\\v{X}^T\\v{X})$ is $n \\times n$ (where $n$ is the number of features)\n",
      "        and the typical algorithms for computing matrix inverses (or pseudo-inverses) are cubic in $n$ \n",
      "        (and even the algorithms with better complexity are all more than quadratic). Inverting a matrix with \n",
      "        tens of thousands of features can be too slow.\n",
      "    </li>\n",
      "    <li>\n",
      "        Gradient Descent is a general method for minimizing a  loss function: it crops up across a wide\n",
      "        range of learning algorithms. (This is why we studied it!) By contrast, the normal equation is \n",
      "        specific to linear regression.\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "There's No Need To Roll Your Own!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Let's make one final point by way of conclusion: of course, you don't need to implement either of these linear\n",
      "    regression methods yourself!\n",
      "    A good library such as Python's scikit-learn algorithm will give you a method for doing it. You've\n",
      "    already seen it done. But here it is again (and I'll use the same two features as above, for comparison):\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['bdrms', 'bthrms']].values\n",
      "y = df['price'].values\n",
      "\n",
      "# Create linear regression object\n",
      "estimator = LinearRegression()\n",
      "\n",
      "# Train the model using the data\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Print the parameters that it learns\n",
      "print('Intercept: ', estimator.intercept_)\n",
      "print('Coefficients: ', estimator.coef_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Intercept: ', -57.668442969719365)\n",
        "('Coefficients: ', array([ 49.06255205,  91.69181314]))\n"
       ]
      }
     ],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}