{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:8749cefbf495bf82eb64bcefda6355e6a45f1a9ce8570cf80c6d30bbed6c54c7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Performance Measures for Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Performance Measures for Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In the previous lecture, we defined <b>classification accuracy</b> as follows:\n",
      "    $$acc = \\frac{1}{|T|}\\sum_{i = 1}^{T} I(\\hat{y}^{(i)} = y^{(i)})$$\n",
      "    It's simply the proportion of correct classifications made over a set of examples $T$.\n",
      "</p>\n",
      "<p>\n",
      "    In this lecture, we explore other measure of classification performance. \n",
      "    Whereas it tends to matter little in the case of regression\n",
      "    (the 'winner' according to one measure of error is typically still the 'winner' if we use\n",
      "    another measure of error), the choice of measure for classification really matters. The 'winner'\n",
      "    can change depending on the choice of measure.\n",
      "</p>\n",
      "<p>\n",
      "    We should choose the measure(s) in advance of running any experiments &mdash; the measure(s) that we think\n",
      "    best suit(s) our problem domain. We should avoid 'fishing expeditions', where we use lots of\n",
      "    measures and then get excited when we perform well on one of them.\n",
      "</p>\n",
      "<p>\n",
      "    The place to start a study of performance measures for classification is the confusion matrix.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Confusion Matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The <b>confusion matrix</b> $CM$ for a classifier is a square $|C| \\times |C|$ matrix. $CM[i, j]$\n",
      "    denotes the number of test examples of class $i$ that were classified as class $j$.\n",
      "</p>\n",
      "<p>\n",
      "    Here are examples of confusion matrices for a binary classifier and a multiclass classifier:\n",
      "</p>\n",
      "<table style=\"float: left\">\n",
      "    <tr>\n",
      "        <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"2\">Predicted Class</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>0</th><th>1</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th rowspan=\"2\">Actual Class</th><th>0</th><td>25</td><td>10</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>1</th><td>20</td><td>45</td>\n",
      "    </tr>\n",
      "</table>\n",
      "<table>\n",
      "    <tr>\n",
      "        <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"3\">Predicted Class</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>0</th><th>1</th><th>2</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th rowspan=\"3\">Actual Class</th><th>0</th><td>10</td><td>0</td><td>15</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>1</th><td>5</td><td>30</td><td>10</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>2</th><td>5</td><td>5</td><td>20</td>\n",
      "    </tr>\n",
      "</table>\n",
      "<p>\n",
      "    Let's assume a test set $T$.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        The sum of all entries in $CM$ equals $|T|$.\n",
      "    </li>\n",
      "    <li>\n",
      "        The sum of the entries in <em>row</em> $i$ is\n",
      "        the number of examples in $T$ that have class $i$.\n",
      "    </li>\n",
      "    <li>\n",
      "        The sum of the entries in <em>column</em> $j$ is\n",
      "        the number of examples in $T$ that the classifier assigns to class $j$.\n",
      "    </li>\n",
      "    <li>\n",
      "        Entries on the main diagonal $CM[i,i]$, are correctly classified, and so $\\sum_i CM[i,i]$ is\n",
      "        the total number of correctly classified examples.\n",
      "    </li>\n",
      "    <li>\n",
      "        Entries off the main diagonal, $CM[i, j], i\\neq j$, are incorrectly classified, and so \n",
      "        $\\sum_i\\sum_{j, j \\neq i} CM[i,j]$ is the total number of incorrectly classified examples.\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Confusion Matrices in scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    scikit-learn produces confusion matrices. Here's the code for the CS1109 dataset:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "# Read CSV file\n",
      "df = pd.read_csv(\"dataset-cs1109.csv\")\n",
      "\n",
      "# Get the feature-values into a separate numpy arrays of numbers and the target values into a separate \n",
      "# numpy arrays of ints\n",
      "X = df[['lect', 'lab', 'cao']].values\n",
      "y = df['outcome'].values\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = np.random)\n",
      "\n",
      "estimator = LogisticRegression()\n",
      "estimator.fit(X_train, y_train)\n",
      "\n",
      "y_predicted = estimator.predict(X_test)\n",
      "cm = confusion_matrix(y_test, y_predicted)\n",
      "\n",
      "cm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([[16, 24],\n",
        "       [ 5, 58]])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Presently, scikit-learn doesn't give any nice way of creating a confusion matrix averaged over \n",
      "    the multiple runs of, e.g. repeated (stratified) holdout or (stratified) $k$-fold cross-validation.\n",
      "</p>\n",
      "<p>\n",
      "    Notwithstanding this limitation, note already how much more revealing the confusion matrix is than\n",
      "    just the classification accuracy: we get insight into the kind of errors that our classifier makes.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Confusion Matrices for Binary Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In the case of <em>binary classification</em>, where we distinguish a positive class from a negative class,\n",
      "    there is some special terminology associated with the cells of the confusion matrix:\n",
      "</p>\n",
      "<table>\n",
      "    <tr>\n",
      "        <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"2\">Predicted Class</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>0</th><th>1</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th rowspan=\"2\">Actual Class</th><th>0</th><td>True Negatives</td><td>False Positives</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>1</th><td>False Negatives</td><td>True Positives</td>\n",
      "    </tr>\n",
      "</table>\n",
      "<ul>\n",
      "    <li>The True Negatives (TN) and True Positives (TP) are correct classifications</li>\n",
      "    <li>The False Negatives (FN) and False Positives FP) are incorrect classifications</li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Calculating Classification Accuracy from a Confusion Matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We gave the definition of classification accuracy above. But it is even easier to define in terms of \n",
      "    the confusion matrix. It is the sum of the entries on the main\n",
      "    diagonal divided by the sum of all entries:\n",
      "    $$acc = \\frac{\\sum_i CM[i,i]}{|T|}$$\n",
      "</p>\n",
      "<p>\n",
      "    Question:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>Compute classification accuracies from the three confusion matrices above.</li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Limitations of Classification Accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Classification accuracy summarizes overall performance in a single figure, which is a good thing. But, because\n",
      "    of this, it has at least three problems:\n",
      "</p>    \n",
      "<ul>\n",
      "    <li>\n",
      "        Giving only a single figure hides information about how the classifier performs on the individual classes.\n",
      "        This problem becomes more acute when the costs of different kinds of misclassification are not equal.\n",
      "        For example, in email classification, it is more serious to misclassify ham as spam.\n",
      "        <p>\n",
      "        <b>Question:</b> Classifier A and classifier B have the same classification accuracy (0.6) but which\n",
      "            would you use?\n",
      "        </p>\n",
      "        <table style=\"float: left\">\n",
      "            <tr>\n",
      "                <th colspan=\"2\" rowspan=\"2\">Classifier A</th><th colspan=\"2\">Predicted Class</th>\n",
      "            </tr>\n",
      "            <tr>\n",
      "                <th>Benign</th><th>Malignant</th>\n",
      "            </tr>\n",
      "            <tr>\n",
      "                <th rowspan=\"2\">Actual Class</th><th>Benign</th><td>400</td><td>100</td>\n",
      "            </tr>\n",
      "            <tr>\n",
      "                <th>Malignant</th><td>300</td><td>200</td>\n",
      "            </tr>\n",
      "        </table>\n",
      "        <table>\n",
      "            <tr>\n",
      "                <th colspan=\"2\" rowspan=\"2\">Classifier B</th><th colspan=\"2\">Predicted Class</th>\n",
      "            </tr>\n",
      "            <tr>\n",
      "                <th>Benign</th><th>Malignant</th>\n",
      "            </tr>\n",
      "            <tr>\n",
      "                <th rowspan=\"2\">Actual Class</th><th>Benign</th><td>200</td><td>300</td>\n",
      "            </tr>\n",
      "            <tr>\n",
      "                <th>Malignant</th><td>100</td><td>400</td>\n",
      "            </tr>\n",
      "        </table>\n",
      "        <p>\n",
      "            In principle, we could assign costs to the different kinds of mis-classification and define\n",
      "            a cost-sensitive variant of classification accuracy. In practice, it's difficult, if not impossible,\n",
      "            to come up with the costs; for example, how much worse is it to classify ham as spam than spam as ham?\n",
      "        </p>\n",
      "        <p>\n",
      "            Instead, we need some other performance measures, such as ones we will look at later in this lecture.\n",
      "        </p>\n",
      "    </li>\n",
      "    <li>\n",
      "        Classification accuracy is also best when the distribution of classes in $T$ is reasonably balanced. If, \n",
      "        on the other hand, some of the classes are more prevalent than others, then they tend to bias the measure, e.g. \n",
      "        if you do well on the more prevalent classes, then you get a higher score overall.\n",
      "        <p>\n",
      "            Consider $T$ that contains 950 positive examples and 50 negative examples. An extremely\n",
      "            effective classifier in terms of classification accuracy for this $T$ is the so-called <b>majority-class classifier</b>,\n",
      "            which we mentioned in the previous lecture, which <em>always</em> predicts\n",
      "            the majority class. In this example, it predicts the positive class, and its classification accuracy is \n",
      "            very high: 0.95. But it isn't really a good classifier: it has no ability to discriminate between\n",
      "            positive and negative examples.\n",
      "       </p>\n",
      "       <p>\n",
      "           Indeed, many people compare the classification accuracy of their classifier(s) against the classification\n",
      "           accuracy of the majority-class classifier to check that they are doing better\n",
      "            than this simple-minded baseline. In case, you want to do this, here's how in\n",
      "            scikit-learn:\n",
      "        </p>\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.dummy import DummyClassifier\n",
      "\n",
      "maj_class_classifier = DummyClassifier(strategy = 'most_frequent')\n",
      "maj_class_classifier.fit(X_train, y_train)\n",
      "y_predicted = maj_class_classifier.predict(X_test)\n",
      "\n",
      "print y_predicted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
        " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
        " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Kappa Statistic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Classification accuracy does not take into account correct classifications from mere chance.\n",
      "    There is a number of measures that <em>correct for chance</em>. They are known as 'agreement statistics'.\n",
      "    We'll look at one of them, the <b>Kappa statistic</b>. It compares your classifier with a random\n",
      "    classifier that predicts the classes as often as your classifier does.\n",
      "</p>\n",
      "<p>\n",
      "    The Kappa statistic is defined as\n",
      "    $$\\kappa = \\frac{acc - rand}{1 - rand}$$\n",
      "</p>\n",
      "<p>\n",
      "    Its values range from -1 to 1.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>If its value is positive, your classifier is doing better than chance.</li>\n",
      "    <li> If its value is zero, then your classifier is doing no better than chance.</li>\n",
      "    <li>If its value is negative, then your classifier is doing worse than chance.</li>\n",
      "</ul>\n",
      "<p>\n",
      "    We'll look separately at how to compute it in the binary and multiclass cases.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Kappa Statistic for Binary Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We know how to define $acc$ in the binary case:\n",
      "    $$acc = \\frac{TN+TP}{|T|}$$\n",
      "    So now what we need is the expected classification accuracy of a classifier that predicts each class as often as your\n",
      "    classifier but does so randomly.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        Our classifier predicts negatives $\\frac{TN+FN}{|T|}$ of the time, and we assume a random classifier that does the \n",
      "        same.\n",
      "    </li>\n",
      "    <li>\n",
      "        Our classifier predicts positives $\\frac{FP+TP}{|T|}$ of the time, and we assume a random\n",
      "        classifier that does the same. \n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    What's the classification accuracy of the random classifier? \n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        It sees negative examples $\\frac{TN+FP}{|T|}$ of the time and classifies $\\frac{TN+FN}{|T|}$ of them \n",
      "        as negative.\n",
      "    </li>\n",
      "    <li>\n",
      "        It sees positive examples $\\frac{FN+TP}{|T|}$ of the time and classifies $\\frac{FP+TP}{|T|}$ \n",
      "        of them as positive. \n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Its classification accuracy is therefore \n",
      "    $$rand =  \\frac{(TN+FP)}{|T|}\\times\\frac{(TN+FN)}{|T|} + \\frac{(FN+TP)}{|T|}\\times\\frac{(FP+TP)}{|T|}$$\n",
      "    And now you can compute $\\kappa$ using the formula given earlier.\n",
      "</p>\n",
      "<p>\n",
      "    Exercise: Compute $\\kappa$ for the binary classifier whose confusion matrix is below (repeated for convenience):\n",
      "</p>\n",
      "<table style=\"float: left\">\n",
      "    <tr>\n",
      "        <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"2\">Predicted Class</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>0</th><th>1</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th rowspan=\"2\">Actual Class</th><th>0</th><td>25</td><td>10</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>1</th><td>20</td><td>45</td>\n",
      "    </tr>\n",
      "</table>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Kappa Statistic for Multiclass Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We know that the classification accuracy of our classifier comes from summing entries on the main diagonal:\n",
      "        $$acc = \\frac{\\sum_i CM[i,i]}{|T|}$$\n",
      "</p>\n",
      "<p>\n",
      "    The key to computing the performance of the random classifier is to realise that \n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        The proportion of examples actually belonging to class $i$ comes from\n",
      "        summing entries in a <em>row</em>, $\\sum_j CM[i,j]$, designated by $CM[i,.]$\n",
      "    </li>\n",
      "    <li>\n",
      "        The proportion of examples your classifier assigns to a class $i$ comes\n",
      "        from summing entries in a <em>column</em>, $\\sum_j CM[j,i]$, designated by $CM[.,i]$\n",
      "    </li>\n",
      "\n",
      "</ul>\n",
      "<p>\n",
      "    So the random classifier's accuracy is given by:\n",
      "    $$rand = \\sum_{i}(\\frac{CM[i,.]}{|T|} \\times \\frac{CM[.,i]}{|T|})$$ \n",
      "</p>\n",
      "<p>\n",
      "    So, in the absence of anything in scikit-learn for computing this, here my own code. You feed it a confusion matrix:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kappa(cm):\n",
      "    num_classes = len(cm)\n",
      "    sum_all = 0\n",
      "    sum_diag = 0\n",
      "    sum_rands = 0\n",
      "    for i in range(0, num_classes):\n",
      "        sum_diag = sum_diag + cm[i, i]\n",
      "        sum_col = 0\n",
      "        sum_row = 0\n",
      "        for j in range(0, num_classes):\n",
      "            sum_col = sum_col + cm[j, i]\n",
      "            sum_row = sum_row + cm[i, j]\n",
      "            sum_all = sum_all + cm[i, j]\n",
      "        sum_rands = sum_rands + sum_row * sum_col\n",
      "    acc = sum_diag * 1.0 / sum_all\n",
      "    rand = sum_rands * 1.0 / (sum_all * sum_all)\n",
      "    return (acc - rand) / (1 - rand)\n",
      "\n",
      "# Call it\n",
      "kappa(cm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.35107538561807516"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "'Silver Standard' Labels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We are taking the labels in the dataset as a 'gold standard': the correct labels. We train using these.\n",
      "    And we test using these &mdash; in particular, our performance measures reward us when the prediction\n",
      "    equals the 'gold standard' and penalise us when they are not equal.\n",
      "</p>\n",
      "<p>\n",
      "    But suppose, as is much more often the case, that the labels in the dataset are what you might call\n",
      "    a 'silver standard', i.e. they are mostly the right labels but sometimes they are not. Labels that\n",
      "    come from a human expert or from some imperfect physical process can be like this. \n",
      "</p>\n",
      "<p>\n",
      "    In some sense, the Kappa statistic adjusts for this, and so this is another reason for using it.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Peformance Measures for Binary Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "True Positive Rate and False Positive Rate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The <b>true positive rate</b> is the proportion of positive examples that are correctly classified. \n",
      "    In terms of the confusion matrix for a binary classifier, it is defined as follows:\n",
      "    $$TPR = \\frac{TP}{TP+FN}$$\n",
      "</p>\n",
      "<p>\n",
      "    Analogously, we can defined the <b>false positive rate</b>, which is the proportion of examples that are \n",
      "    classified as positive but which are not actually positive:\n",
      "    $$FPR = \\frac{FP}{FP+TN}$$\n",
      "</p>\n",
      "<p>\n",
      "    Question:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>What is the TPR and FPR of the binary classifier whose confusion matrix was repeated above?</li>\n",
      "</ul>\n",
      "<p>\n",
      "    We can position a classifer in <b>ROC space</b>. ROC stands for 'Receiver Operating Characteristic'. In ROC\n",
      "    space, the TPR (between 0 and 1) is on the vertical axis and the FPR (also between 0 and 1) is on the horizontal\n",
      "    axis. Here's an example with a few imaginary classifiers plotted on it:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(5,5))\n",
      "plt.title(\"ROC space\")\n",
      "plt.xlabel(\"FPR\")\n",
      "plt.xlim(0, 1)\n",
      "plt.ylabel(\"TPR\")\n",
      "plt.ylim(0, 1)\n",
      "plt.plot([0, 1], [1, 0], linestyle = 'dotted', color = 'gray')\n",
      "plt.plot([0, 1], [0, 1], linestyle = 'dashed', color = 'gray')\n",
      "plt.annotate(\"Trivial\", xy = (0, 0), xytext = (0.02,0.1), arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "plt.annotate(\"Trivial\", xy = (1, 1), xytext = (0.9,0.9), arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "plt.annotate(\"Best\", xy = (0, 1), xytext = (0.02,0.9), arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "plt.annotate(\"Worst\", xy = (1, 0), xytext = (0.9,0.1), arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "plt.annotate(\"Worse than random\", xy = (0.8, 0.2), xytext = (0.5,0.1), arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "plt.annotate(\"OK\", xy = (0.4, 0.6), xytext = (0.3,0.5), arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "plt.annotate(\"Better\", xy = (0.2, 0.7), xytext = (0.1,0.6), arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAFRCAYAAAAb00QnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fb8d7e73c50>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ul>\n",
      "    <li>\n",
      "        The diagonal from bottom-left to top-right has TPR = FPR. Classifiers along this line are in some sense random.\n",
      "        They are like a biased coin whose probability of predicitng the positive class is TPR.\n",
      "    </li>\n",
      "    <li>\n",
      "        Hence, classifiers that lie above the line are better than random.\n",
      "    </li>\n",
      "    <li>\n",
      "        And those lying below the line are worse than random.\n",
      "    </li>\n",
      "    <li>\n",
      "        In general, a classifier A is better than classifier B if A is to the left and higher than B.\n",
      "    </li>\n",
      "    <li>\n",
      "        The point (0, 0) is a trivial classifier: what does it do?\n",
      "    </li>\n",
      "    <li>\n",
      "        The point (1, 1) is a trivial classifier: what does it do?\n",
      "    </li>\n",
      "    <li>\n",
      "        The point (0, 1) is the best possible: it gets all the positives right and makes no mistakes on the negatives.\n",
      "    </li>\n",
      "    <li>\n",
      "        The point (1, 0) is the worst possible: it gets all predictions wrong.\n",
      "    </li>\n",
      "    <li>\n",
      "        The diagonal from top-left to bottom-right has TPR = 1 - FPR. Classifiers along this line perform equally well on\n",
      "        positives and negatives.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Note the diagram does <strong>not</strong> show a ROC curve, which we will not cover except...except to say that a\n",
      "    ROC curve is drawn for a classifier that has a threshold (e.g. logistic regression). You adjust the threshold and\n",
      "    plot multiple points in ROC space. Classifiers are sometimes measured by the area under the ROC curve.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Precision and Recall"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The measures we look at now have a focus on one of the classes, usually the positive class in binary\n",
      "    classification. (However, in multiclass classification, you could compute these measures repeatedly,\n",
      "    treating each class as the 'positive' class in turn. But we won't look at that here.)\n",
      "</p>\n",
      "<p>\n",
      "    Given that one class is 'special' (the positive class), we might like to know the proportion of \n",
      "    examples assigned to this class that are correctly classified. This is known as <b>precision</b>:\n",
      "    $$precision = \\frac{TP}{TP+FP}$$\n",
      "</p>\n",
      "<p>\n",
      "    And similarly, we might like to know the proportion of examples of this class that are correctly\n",
      "    classified. This is known as <b>recall</b>:\n",
      "    $$recall = \\frac{TP}{TP+FN}$$\n",
      "</p>\n",
      "<p>\n",
      "    In effect, we are dividing TP by:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>the sum of the column i.e. predictions of this class, in the case of precision, or</li>\n",
      "    <li>the sum of the row, i.e. examples of this class, in the case of recall</li>\n",
      "</ul>\n",
      "<p>\n",
      "    Having two measures can make it more difficult to compare classifiers. Suppose, for example, that one\n",
      "    classfier has better precision and the other has better recall. Which is the better classifier?\n",
      "</p>\n",
      "<p>\n",
      "    To make comparison easier, precision and recall are sometimes combined into a single number.\n",
      "    The most common measure is <b>$F_1$</b>:\n",
      "    $$F_1 = \\frac{2 \\times precision \\times recall}{precision + recall}$$\n",
      "    This is the harmonic mean of the two numbers. In the form given here, it gives equal weight to precision\n",
      "    and recall. It is also possible to define a weighted harminc mean, allowing you to put more or less weight \n",
      "    on precision. But, rarely is it clear what weight to use.\n",
      "</p>\n",
      "<p>\n",
      "    Exercise:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        What are the precision, recall and $F_1$ values for the binary classifier whose confusion matrix was \n",
      "        given earlier?\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Finally, here are all the measures for kNN (with $k$ arbitrarily set to 3 and unweighted) and logistic\n",
      "    regression:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.metrics import f1_score\n",
      "\n",
      "estimators = {\"Logistic Regression\" : LogisticRegression(), \"kNN\" : KNeighborsClassifier(n_neighbors = 3)}\n",
      "for name, estimator in estimators.items():\n",
      "    print name\n",
      "    estimator.fit(X_train, y_train)\n",
      "    y_predicted = estimator.predict(X_test)\n",
      "    cm = confusion_matrix(y_test, y_predicted)\n",
      "    print \"CM:\", cm\n",
      "    print \"Accuracy:\", accuracy_score(y_test, y_predicted)\n",
      "    print \"Kappa:\", kappa(cm)\n",
      "    print \"TPR:\", cm[1, 1] * 1.0 / (cm[1, 0] + cm[1, 1]) # nasty hack, relying on binary classifier\n",
      "    print \"FPR:\", cm[0, 1] * 1.0 / (cm[0, 0] + cm[0, 1]) # ditto\n",
      "    print \"Precision:\", precision_score(y_test, y_predicted)\n",
      "    print \"Recall\", recall_score(y_test, y_predicted)\n",
      "    print \"F1\", f1_score(y_test, y_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kNN\n",
        "CM: [[26 14]\n",
        " [13 50]]\n",
        "Accuracy: 0.73786407767\n",
        "Kappa: 0.445684672115\n",
        "TPR: 0.793650793651\n",
        "FPR: 0.35\n",
        "Precision: 0.78125\n",
        "Recall 0.793650793651\n",
        "F1 0.787401574803\n",
        "Logistic Regression\n",
        "CM: [[16 24]\n",
        " [ 5 58]]\n",
        "Accuracy: 0.718446601942\n",
        "Kappa: 0.351075385618\n",
        "TPR: 0.920634920635\n",
        "FPR: 0.6\n",
        "Precision: 0.707317073171\n",
        "Recall 0.920634920635\n",
        "F1 0.8\n"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}