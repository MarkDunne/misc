{
 "metadata": {
  "name": "",
  "signature": "sha256:ee79b2a104c427fecc82084fcaa85a8c48a6dbb8e40323345ef4364123f10e9b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    As previously discussed,\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        <b>Regression</b> means predicting a continuous value.\n",
      "    </li>\n",
      "    <li>\n",
      "        <b>Classification</b> means predicting a discrete value.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    In classification, we are given an object and we predict to which of a finite (and usually small) set of classes the\n",
      "    object belongs. So we formulate it as follows:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        We assume we have a finite set of <b>labels</b>, $\\cal{C}$.\n",
      "    </li>\n",
      "    <li>\n",
      "        Given an object $\\v{x}$, our task is to assign one of the labels $\\hat{y} \\in \\cal{C}$ to the object.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    We will often use integers for the labels.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        For example, given an email, a spam filter predicts $\\hat{y} \\in \\Set{0,1}$, where 0 means ham and 1 means spam. \n",
      "    </li>\n",
      "    <li>\n",
      "        But a classifier should not treat these as continuous, e.g. it should never output 0.5.\n",
      "    </li>\n",
      "    <li>\n",
      "        Furthermore, where there are more than two labels, we should not assume a relationship\n",
      "        between the labels. \n",
      "        <ul>\n",
      "            <li>\n",
      "                Suppose there are three classes $\\Set{1,2,3}$ and we are classifying object $\\v{x}$. One model predicts\n",
      "                $\\hat{y} = 1$ and another model predicts $\\hat{y} = 2$. But the actual label for $\\v{x}$ is $y = 3$. Which model has done better?\n",
      "            </li>\n",
      "            <li>\n",
      "                This problem of the unrelatedness of the labels in the case where there are more than two\n",
      "                labels is one reason why we can't use regression directly for classification.\n",
      "            </li>\n",
      "        </ul>\n",
      "     </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "A Variation of Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Given an object $\\v{x}$, a classifier outputs a label, $\\hat{y} \\in \\cal{C}$. Instead, a classifer could output a \n",
      "    probability distribution over the labels $\\cal{C}$.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        E.g. given an email, a spam filter might output $\\Tuple{0.2, 0.8}$ meaning $P(y = \\mathit{ham} | \\v{x}) = 0.2$ and $P(y = \\mathit{spam}| \\v{x}) = 0.8$.\n",
      "    </li>\n",
      "    <li>\n",
      "        The probabilities must sum to 1.\n",
      "    </li>\n",
      "    <li>\n",
      "        We can convert such a classifier into a more traditional one by taking the probability distribution\n",
      "        and selecting the class with the highest probability:\n",
      "        $$\\arg \\max_{\\hat{y} \\in \\cal{C}} P(\\hat{y}| \\v{x})$$\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Types of Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We distinguish two types of classification:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        <b>binary classification</b>, in which there are just two classes, i.e. $|\\cal{C}| = 2$, e.g. fail/pass, ham/spam, benign/malignant;\n",
      "    </li>\n",
      "    <li>\n",
      "        <b>multiclass classification</b>, where there are more than two classes, i.e. $|\\cal{C}| > 2$, e.g. \n",
      "        let's say that a post to a forum can be a question, an answer, a clarification or an irrelevance.\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Binary classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In binary classification, it is common to refer to one class (the one labelled 0) as the <b>negative class</b>\n",
      "    and the other (the one labelled 1) as the <b>positive class</b>. It doesn't really matter which is which. But, usually, \n",
      "    we treat the class we're trying to identify, or the class that requires special action, as the positive class.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        So in spam filtering, ham is the negative class; spam is the positive class.\n",
      "    </li>\n",
      "    <li>\n",
      "        What about tumour classification?\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    This terminology is extended to other things too, e.g. we can refer to <b>negative examples</b> and\n",
      "    <b>positive examples</b>.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiclass classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Note that many classifiers can only do binary classification. How can we use a binary classifier for a multiclass\n",
      "    problem? Two methods are common:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>One-versus-Rest; and </li>\n",
      "    <li>One-versus-One.</li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "One-versus-Rest"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    <b>One-versus-Rest</b>  involves training $|C|$ classifiers, one per class. It relies on the idea that these classifiers\n",
      "    can ouput probabilities, as above. You can think of it as taking the\n",
      "    training set, producing $|C|$ copies of it but with modified labels, and training a classifier on each copy, as follows:\n",
      "</p>\n",
      "<ul style=\"background: lightgray\">\n",
      "    <li>\n",
      "        <b>for each</b> class $c \\in C$\n",
      "        <ul>\n",
      "            <li>\n",
      "                create a copy of the training set in which you replace examples $\\Tuple{\\v{x}, c}$ by $\\Tuple{\\v{x}, 1}$\n",
      "                and examples $\\Tuple{\\v{x}, c'}$ where $c' \\neq c$ by $\\Tuple{\\v{x}, 0}$\n",
      "            </li>\n",
      "            <li>\n",
      "                train classifier $L_c$ on this modified training set\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    After all these classifiers have been trained, to classify $\\v{x}$, you run all the classifiers $L_c$ for each $c \\in C$.\n",
      "    The predicted class of $\\v{x}$ is the class $c$ whose classifier $L_c$ predicts 1 with highest probability.\n",
      "</p>\n",
      "<p>\n",
      "    This is a good approach provided the binary classifer is one that produces reliable probabilities. \n",
      "</p>\n",
      "<p>\n",
      "    Another name for this is 'One-versus-All'. We will stick with One-versus-Rest on the basis that this is the name of a scikit-learn class that can do this for us.\n",
      "</p>\n",
      "<p>\n",
      "    There are some variations of One-versus-Rest, e.g. the use of error-correcting codes. We will not study these\n",
      "    because they are more complicated but also there are restrictions on their applicability (e.g. there need\n",
      "    to be at least four classes and there may be problems using them for kNN classifiers).\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "One-versus-One"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In <b>One-versus-One</b>, you build a classifier <em>for every pair</em> of classes using only the training data for those \n",
      "    two classes. \n",
      "</p>\n",
      "<p>\n",
      "    After all these classifiers have been trained, when you want to classify $\\v{x}$, you run all the classifiers and,\n",
      "    for each class $c \\in C$, you count how many of the classifiers predict that class. The predicted class of $\\v{x}$ \n",
      "    is the one that is predicted most often.\n",
      "</p>\n",
      "<p>\n",
      "    One-versus-One's advantage over One-versus-Rest is that the individual classifiers do not need to produce\n",
      "    probabilities. But the disadvantage is the number of individual classifiers it must train.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>In terms of $|C|$, how many?</li>\n",
      "    <li>This sounds like a lot of work. What offsets this, so that in practice it may be quite feasible?</li>\n",
      "</ul>\n",
      "<p>\n",
      "    Sometimes, even when you have a classifier that can directly handle multiclass classification, using it\n",
      "    in a One-versus-One fashion can be more accurate! This is because it is like an 'ensemble' learning method,\n",
      "    which we will discuss properly in a later lecture.\n",
      "</p>\n",
      "<p>\n",
      "    Here again there is another name in the literature, 'pairwise classification', but we stick with scikit-learn's\n",
      "    nomenclature.\n",
      "</p>\n",
      "<p>\n",
      "    There are some other methods with similarities to One-versus-One, such as ensembles of nested dichotomies,\n",
      "    which we will not study in this introductory module.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiclass classification in scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    scikit-learn provides classes that make it easy to implement One-versus-Rest, One-versus-One and \n",
      "    error-correcting codes. But we don't need them!\n",
      "</p>\n",
      "<p>\n",
      "    Even when a classification algorithm can only do binary classification, scikit-learn's implementation\n",
      "    comes with one of the above methods built-in already. For example, we will see that logistic regression\n",
      "    typically only does binary classification, but scikit-learn's implementation of it already makes use of\n",
      "    One-versus-Rest. \n",
      "</p>\n",
      "<p>\n",
      "    As we proceed, I will make clear which classifiers are restricted to binary classification. But,\n",
      "    from an implementation point of view, we can ignore this issue from now on.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Other Types of Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Here are two variations of classification that we will not be studying any further in this module.\n",
      "</p>\n",
      "<p>\n",
      "    In <b>multilabel classification</b>, the classifier can assign $\\v{x}$ to more than one class.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        In other words, it outputs a <em>set</em> of labels, $\\hat{y} \\subseteq \\cal{C}$.\n",
      "    </li>\n",
      "    <li>\n",
      "        E.g. consider a movie classifier where the classes are genres, e.g. $\\cal{C} = \\Set{\\mathit{comedy},\n",
      "        \\mathit{action}, \\mathit{horror}, \\mathit{musical}, \\mathit{romance}}$. \n",
      "        <ul>\n",
      "            <li>\n",
      "                The classifier's output for <i>The Blues Brothers</i> should be $\\Set{\\mathit{comedy},\n",
      "                \\mathit{action}, \\mathit{musical}}$.\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "    <li>\n",
      "        Do <strong>not</strong> confuse this with <em>multiclass</em> classification\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    In <b>ordered classification</b>, there is an <em>ordering</em> defined on the classes. The ordering matters\n",
      "    in measuring the performance of the classifier.\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        E.g. consider a classifier that predicts a student's degree class, i.e. $\\cal{C} = \\Set{\\mathit{Ordinary},\n",
      "        \\mathit{3rd}, \\mathit{2ii}, \\mathit{2i}, \\mathit{1st}}$.\n",
      "    </li>\n",
      "    <li>\n",
      "        Suppose we are classifying student $\\v{x}$. One model predicts $\\hat{y} = \\mathit{2ii}$ and another model predicts\n",
      "        $\\hat{y} = \\mathit{2i}$. But the actual label for $\\v{x}$ is $y = \\mathit{1st}$. Which model has done better?\n",
      "    </li>\n",
      "<ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Nearest-Neighbours for Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Recall the CS1109 dataset: it has 342 examples of students; 3 features ($\\mathit{lect}$, $\\mathit{lab}$,\n",
      "    $\\mathit{cao}$); the dependent variable's values are either 0 (the student fails) or 1 (the student\n",
      "    passes). Let's load it and, because we'll be using nearest-neighbour methods, let's scale it:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read CSV file\n",
      "df = pd.read_csv(\"dataset-cs1109.csv\")\n",
      "\n",
      "# lect, where min = 0 and max = 100 (it's a percentage)\n",
      "# Subtract the min\n",
      "# df['lect'] -= 0\n",
      "# Divide by the range, i.e. max - min\n",
      "df['lect'] /= 100\n",
      "\n",
      "# lab, where min = 0 and max = 100 (it's a percentage)\n",
      "# Subtract the min\n",
      "# df['lab'] -= 0\n",
      "# Divide by the range, i.e. max - min\n",
      "df['lab'] /= 100\n",
      "\n",
      "# cao, where min = 200 and max = 600\n",
      "# Subtract the min\n",
      "df['cao'] -= 200\n",
      "# Divide by the range, i.e. max - min\n",
      "df['cao'] /= 400\n",
      "\n",
      "# Get the feature-values into a separate numpy arrays of numbers and the target values into a separate \n",
      "# numpy arrays of ints\n",
      "X = df[['lect', 'lab', 'cao']].values\n",
      "y = df['outcome'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "1NN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In 1NN, we predict the class of the nearest-neighbour. We'll do this for Craig, who is described by this vector \n",
      "    $\\cv{60\\\\45\\\\500}$ but, when scaled, is described by this vector $\\cv{0.6\\\\0.45\\\\0.75}$\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "estimator = KNeighborsClassifier(n_neighbors = 1)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "estimator.predict([[0.6, 0.45, 0.75]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([1])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    His nearest-neighbour is $\\Tuple{\\cv{0.39\\\\0.5\\\\0.7}, 1}$ (scaled), which is $\\Tuple{\\cv{39\\\\50\\\\480}, 1}$ \n",
      "    (unscaled). We predict 1 &mdash; he will pass.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "kNN"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Unweighted"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    With $k > 1$, for regression we returned the mean of the neighbours' $y$-values.\n",
      "</p>\n",
      "<p>\n",
      "    Question:\n",
      "<ul>\n",
      "    <li>What will we do for classification?</li>\n",
      "    <li>\n",
      "        Hence, for unweighted kNN ($k > 1$), if someone picks $k$ manually (rather than using model selection), s/he \n",
      "        often chooses an odd number, e.g. $k = 3$, $k = 5$. Why?\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = KNeighborsClassifier(n_neighbors = 3)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "estimator.predict([[0.6, 0.45, 0.75]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array([0])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Craig's 3 nearest-neighbours are $\\Tuple{\\cv{0.39\\\\0.5\\\\0.7}, 1}$, $\\Tuple{\\cv{0.48\\\\0.36\\\\0.575}, 0}$, and \n",
      "    $\\Tuple{\\cv{0.68\\\\0.64\\\\0.6375}, 0}$. Hence, now we predict 0 (fail).\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "kNN"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Distance-weighted"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In distance-weighted kNN, the closer you are, the greater your vote.\n",
      "</p>\n",
      "<p>\n",
      "    The distances of the neighbours from Craig, to three decimal places, are 0.222, 0.230 and 0.234, respectively. The\n",
      "    weights are $1/(0.0001 + 0.222) = 4.502$, $1/(0.0001 + 0.230) = 4.346$ and $1/(0.0001 + 0.234) = 4.271$ respectively.\n",
      "    So the votes for class 0 ($\\mathit{fail}$) are $4.346 + 4.271 = 8.617$; and the votes for class 1 ($\\mathit{pass}$) \n",
      "    are $4.502$. So we predict Craig fails.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def inv_distances(dists):\n",
      "    return 1 / (0.0001 + dists)\n",
      "\n",
      "estimator = KNeighborsClassifier(n_neighbors = 3, weights = inv_distances)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "estimator.predict([[0.6, 0.45, 0.75]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([0])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "kNN"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Shepard's Method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Here $k = m =342$ and we must use distance-weighting. (Why?)\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = KNeighborsClassifier(n_neighbors = 342, weights = inv_distances)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "estimator.predict([[0.6, 0.45, 0.75]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([1])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    So now we're once more predicting he passes!\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "kNN"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    What if we want to use nearest-neighbour methods to build the kind of classifier that outputs a probability\n",
      "    distribution?\n",
      "</p>\n",
      "<ul>\n",
      "    <li>We can't do this when $k = 1$. Why not?</li>\n",
      "    <li>In the unweighted 3NN example above, the probabilities would be $\\Tuple{\\frac{2}{3}, \\frac{1}{3}}$ meaning\n",
      "    $P(\\mathit{fail} | \\v{x}) = 0.66$ and $P(\\mathit{pass}| \\v{x}) = 0.33$. How are these computed?</li>\n",
      "    <li>In the weighted 3NN example above, the probabilities would be $\\Tuple{0.657, 0.343}$ meaning\n",
      "    $P(\\mathit{fail} | \\v{x}) = 0.657$ and $P(\\mathit{pass}| \\v{x}) = 0.343$ How are these computed?</li>\n",
      "</ul>\n",
      "<p>\n",
      "    What about multiclass classification?\n",
      "</p>\n",
      "<ul>\n",
      "    <li>Can nearest-neighbour classifiers be used for multiclass classification?</li>\n",
      "</ul>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}