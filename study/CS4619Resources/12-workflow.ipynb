{
 "metadata": {
  "name": "",
  "signature": "sha256:e7ed480f91384efadb5884eda648c128e5137459f0060b6c9a6b785d20b09255"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Workflow"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Workflow"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    This lecture tries to bring the previous lectures together by showing a complete workflow for a simple example.\n",
      "    Specifically, we will exemplify the following topics in the case of the CorkB dataset:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>Data preparation;</li>\n",
      "    <li>Error estimation (in the case where there are hyperparameters);</li>\n",
      "    <li>Model selection; and</li>\n",
      "    <li>Deployment.</li>\n",
      "</ul>\n",
      "<p>\n",
      "    One new thing to watch out for is the use of scikit-learn pipelines, which we will use in order to impute means\n",
      "    and modes and to standardize data, without leakage.\n",
      "</p>\n",
      "<p>\n",
      "    Obviously, you start by reading in your data:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"dataset-corkB.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Preparation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "   Next, you should do such things as the following:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        You should get to know your data by: displaying the column names; displaying the shape; displaying the data \n",
      "        types; running the pandas <code>describe</code> function; plotting graphs; and so on. We won't show the Python \n",
      "        for that again here.\n",
      "    </li>\n",
      "    <li>\n",
      "        If your dataset uses strings such as '?', 'N/A', 'UNK', etc. to represent missing values, you might consider\n",
      "        replacing these strings by the float NaN. This doesn't apply to the CorkB dataset.\n",
      "    </li>\n",
      "    <li>\n",
      "        You should detect and remove anomalies &mdash; for the CorkB dataset, we will remove examples with\n",
      "        anomalous-looking $\\mathit{flarea}$ values. \n",
      "    </li>\n",
      "    <li>\n",
      "        You should consider what to do about examples that have features whose values are missing. We disussed the options\n",
      "        previously. This might result in deleting examples, or even deleting whole features. We'll exemplify that\n",
      "        here by deleting examples from the CorkB dataset whose price is missing. \n",
      "    </li>\n",
      "    <li>\n",
      "        You should change nominal-valued features into numeric-values features, e.g. using binary encoding or one-hot\n",
      "        encoding.\n",
      "    </li>\n",
      "    <li>\n",
      "        If you want to scale values using min-max scaling, then now is the time to do it (assuimg the min and max\n",
      "        are provided by your domain expert). (But if you want to scale values using standardization, now is not \n",
      "        the time to do it.) We will not do any min-max scaling on the CorkB dataset.\n",
      "     </li>\n",
      "</ul>\n",
      "<p>\n",
      "    What you should not do at this stage is anything that involves computing values from the dataset as a whole,\n",
      "    such as computing means or modes to impute missing values, or computing means and standard deviations to\n",
      "    standardize values. Remember, doing these things on the whole dataset results in leakage.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remove anomalies from flarea\n",
      "df = df[(df['flarea'].isnull()) | ((df['flarea'] > 10) & (df['flarea'] < 1000))]\n",
      "df.reset_index(drop=True, inplace=True)\n",
      "# Remove examples with missing prices\n",
      "df.dropna(subset=['price'], inplace=True)\n",
      "df.reset_index(drop=True, inplace=True)\n",
      "# Encode nominal-valued features\n",
      "# - devment is easy because it is binary-valued\n",
      "df.replace({'devment' : {'SecondHand' : 0, 'New' : 1}}, inplace=True)\n",
      "# - for type we use one-hot encoding\n",
      "one_hot = pd.get_dummies(df['type'], 'type', '_')\n",
      "df.drop('type', axis=1, inplace=True)\n",
      "df = pd.concat([df, one_hot], axis=1)\n",
      "# - similarly for ber\n",
      "one_hot = pd.get_dummies(df['ber'], 'ber', '_')\n",
      "df.drop('ber', axis=1, inplace=True)\n",
      "df = pd.concat([df, one_hot], axis=1)\n",
      "# - and similarly for location\n",
      "one_hot = pd.get_dummies(df['location'], 'location', '_')\n",
      "df.drop('location', axis=1, inplace=True)\n",
      "df = pd.concat([df, one_hot], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We must remember that we still have some missing values to deal with, and we would\n",
      "    like to standardize the data too.\n",
      "</p>\n",
      "<p>\n",
      "    It turns out that there will be a problem when it comes to the missing values.\n",
      "    scikit-learn's class for doing this is somewhat brute force. It will either replace\n",
      "    NaNs by the mean or by the mode or by the median. What it will not allow us to do\n",
      "    is to replace one feature's missing values by the mean, and another's by the mode.\n",
      "    Yet, this is exactly what we would like to be able to to. E.g. we want to replace\n",
      "    NaNs for $\\mathit{flarea}$ by the mean floor area; and we want to replace NaNs for\n",
      "    $\\mathit{devment}$ by the mode. Why?\n",
      "</p>\n",
      "<p>\n",
      "    Furthermore, it seems that <code>Imputer</code> does not play nicely with scikit-learn\n",
      "    pipelines. The input to a pipeline is checked to see whether it contains NaNs and, if\n",
      "    it does, you get an error &mdash; even though the pipeline might contain an Imputer\n",
      "    whose job it is to get rid of the NaNs.\n",
      "</p>\n",
      "<p>\n",
      "    My hack to make this work is, to replace NaNs as this stage by some other values.\n",
      "    Where we want to use the mean, replace the NaNs by one value; where we want to use\n",
      "    the mode, replace the NaNs by some other value. These values must be special values, \n",
      "    different from all other values in the dataset. \n",
      "    For $\\mathit{devment}$, we will replace NaNs by -2 and then for all other NaNs, we will\n",
      "    replace by -1. Why will this do what we want?\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['devment'].fillna(-2, inplace=True)\n",
      "df.fillna(-1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Finally, extract the data:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = df[['flarea', 'bdrms', 'bthrms', 'floors', 'devment', \n",
      "        'type_Apartment', 'type_Detached', 'type_Semi-detached', 'type_Terraced', \n",
      "        'ber_B1', 'ber_B2', 'ber_B3', 'ber_C1', 'ber_C2', 'ber_C3', 'ber_D1', 'ber_D2', 'ber_E1', 'ber_E2', 'ber_F', 'ber_G',\n",
      "        'location_Ballinlough', 'location_Ballintemple', 'location_Ballyphehane', 'location_Ballyvolane', \n",
      "        'location_Banduff', 'location_Bishopstown', 'location_Blackpool', 'location_Blackrock', 'location_Carrigrohane', \n",
      "        'location_CityCentre', 'location_Cloghroe', 'location_Donnybrook', 'location_Douglas', 'location_DublinPike', \n",
      "        'location_Farranree', 'location_Fota', 'location_Glanmire', 'location_Glasheen', 'location_Grange', \n",
      "        'location_Gurranabraher', 'location_Inniscarra', 'location_Mayfield', 'location_ModelFarmRoad', \n",
      "        'location_Montenotte', 'location_Ovens', 'location_PassageWest', 'location_Rochestown', 'location_Silversprings', \n",
      "        'location_StLukes', 'location_SundaysWell', 'location_TheLough', 'location_Togher', 'location_TurnersCross', \n",
      "        'location_VictoriaCross', 'location_Waterfall', 'location_WesternRoad', 'location_Wilton']].values\n",
      "y = df['price'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Estimators"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    You should now decide which estimators you would like to compare. For the CorkB dataset,\n",
      "    let's compare OLS linear regression, lasso regression, ridge regression, and \n",
      "    distance-weighted kNN.\n",
      "</p>\n",
      "<p>\n",
      "    We create these estimators. As we know, these estimators will be trained on the training \n",
      "    examples (using scikit-learn's <code>fit</code> method) and tested on the test examples\n",
      "    (using scikit-learn's <code>predict</code> method).\n",
      "</p>\n",
      "<p>\n",
      "    But, as part of the training, we want to compute means and modes from the <em>training\n",
      "    examples only</em>. Then we must impute these means and modes for the missing values in the\n",
      "    training examples and in the test examples. \n",
      "    Fortunately, this is what scikit-learn's <code>Imputer</code> class is designed\n",
      "    to do. It has a <code>fit</code> method, which computes means or modes from the training\n",
      "    data; then it has a <code>transform</code> method, which can be invoked either on the\n",
      "    training examples or the test examples, to replace the missing values by the means or modes\n",
      "    computed from the training examples.\n",
      "</p>\n",
      "<p>\n",
      "    scikit-learn's <code>StandardScaler</code> class is similar. It has a <code>fit</code>\n",
      "    method, which you can use to compute means and standard deviations from the training\n",
      "    examples. Then it has a <code>transform</code> method, which can be invoked either on\n",
      "    the training examples or the test examples, in order to standardize the values using the\n",
      "    means and standard deviations computed from the training examples.\n",
      "</p>\n",
      "<p>\n",
      "    We will use scikit-learn's <code>Pipeline</code> class, which allows us to stick together\n",
      "    a sequence of transformations (such as an <code>Imputer</code> and a \n",
      "    <code>StandardScaler</code>) plus, optionally, a final estimator.\n",
      "</p>\n",
      "<p>\n",
      "    When we invoke the \n",
      "    <code>Pipeline</code>'s <code>fit</code> method on the training examples, it runs the\n",
      "    <code>fit</code> methods and <code>transform</code> methods of each transformation in the \n",
      "    <code>Pipeline</code>, and it runs the <code>fit</code> method of the estimator in the\n",
      "    <code>Pipeline</code>.\n",
      "</p>\n",
      "<p>\n",
      "    When we\n",
      "    invoke the <code>Pipeline</code>'s <code>predict</code> method on the test examples,\n",
      "    it runs the <code>tranform</code> methods of each transformation in the \n",
      "    <code>Pipeline</code>, and it runs the <code>predict</code> method of the estimator in the\n",
      "    <code>Pipeline</code>.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn.linear_model import RidgeCV\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "\n",
      "ols = Pipeline([('impute_means', Imputer(missing_values=-1, strategy='mean')), \n",
      "                ('impute_modes', Imputer(missing_values=-2, strategy='most_frequent')),\n",
      "                ('standardize', StandardScaler()), \n",
      "                ('estimator', LinearRegression())])\n",
      "\n",
      "lassocv = Pipeline([('impute_means', Imputer(missing_values=-1, strategy='mean')), \n",
      "                ('impute_modes', Imputer(missing_values=-2, strategy='most_frequent')),\n",
      "                ('standardize', StandardScaler()), \n",
      "                ('estimator', LassoCV(cv=10))])\n",
      "\n",
      "ridgecv = Pipeline([('impute_means', Imputer(missing_values=-1, strategy='mean')), \n",
      "                ('impute_modes', Imputer(missing_values=-2, strategy='most_frequent')),\n",
      "                ('standardize', StandardScaler()), \n",
      "                ('estimator', RidgeCV(cv=10))])\n",
      "\n",
      "def inv_distances(dists):\n",
      "    return 1 / (0.0001 + dists)\n",
      "\n",
      "knn = Pipeline([('impute_means', Imputer(missing_values=-1, strategy='mean')), \n",
      "                ('impute_modes', Imputer(missing_values=-2, strategy='most_frequent')),\n",
      "                ('standardize', StandardScaler()), \n",
      "                ('estimator', KNeighborsRegressor(weights = inv_distances))])\n",
      "knn_hyperparameters = {'estimator__n_neighbors' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Questions: \n",
      "</p>\n",
      "<ul>\n",
      "    <li>Suppose we call <code>ols.fit(X_train, y_train)</code> on some training data. \n",
      "        How many methods will this invoke?\n",
      "    </li>\n",
      "    <li>Suppose we then call <code>ols.predict(X_test)</code> on some test data.\n",
      "        How many methods will this invoke?\n",
      "    </li>\n",
      "    <li>\n",
      "        Why are we doing OLS differently from Lasso and Ridge above, and why is kNN different again?\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Error Estimation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "ols_mses_test = np.abs(cross_val_score(ols, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "ols_mean_mse_test = np.mean(ols_mses_test)\n",
      "\n",
      "lassocv_mses_test = np.abs(cross_val_score(lassocv, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "lassocv_mean_mse_test = np.mean(lassocv_mses_test)\n",
      "\n",
      "ridgecv_mses_test = np.abs(cross_val_score(ridgecv, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "ridgecv_mean_mse_test = np.mean(ridgecv_mses_test)\n",
      "\n",
      "knn_gs = GridSearchCV(knn, knn_hyperparameters, scoring = 'mean_squared_error', cv = 10) \n",
      "knn_mses_test = np.abs(cross_val_score(knn_gs, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "knn_mean_mse_test = np.mean(knn_mses_test)\n",
      "\n",
      "print('OLS %4f\\nLasso %4f\\nRidge %4f\\nkNN %4f' % \n",
      "      (ols_mean_mse_test, lassocv_mean_mse_test, ridgecv_mean_mse_test, knn_mean_mse_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "OLS 1009502822020569413526887596032.000000\n",
        "Lasso 81472.262069\n",
        "Ridge 86649.758397\n",
        "kNN 118559.234866\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We have a clear winner: Lasso. So now we must find the best value for its hyperparameter, $\\lambda$:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lassocv.fit(X, y)\n",
      "\n",
      "lassocv.get_params()['estimator'].alpha_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "5.0750946535713855"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    If the winner had been kNN, then we would need to find the best value for $k$:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs = GridSearchCV(knn, knn_hyperparameters, scoring = 'mean_squared_error', cv = 10)\n",
      "gs.fit(X, y)\n",
      "\n",
      "gs.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "{'estimator__n_neighbors': 2}"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    What would you have done if OLS had been the best model?\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Deployment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    So now it is decided: we are going to use Lasso with $\\lambda = 5.075$. It makes sense to build this\n",
      "    system from all the data that we have available:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Lasso\n",
      "\n",
      "final_model = Pipeline([('impute_means', Imputer(missing_values='NaN', strategy='mean')), \n",
      "                ('impute_modes', Imputer(missing_values=-1, strategy='most_frequent')),\n",
      "                ('standardize', StandardScaler()), \n",
      "                ('estimator', Lasso(alpha = 5.075))])\n",
      "\n",
      "final_model.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "Pipeline(steps=[('impute_means', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('impute_modes', Imputer(axis=0, copy=True, missing_values=-1, strategy='most_frequent',\n",
        "    verbose=0)), ('standardize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', Lasso(alpha=5.075, copy_X=True, fit_intercept=True, max_iter=1000,\n",
        "   normalize=False, positive=False, precompute='auto', tol=0.0001,\n",
        "   warm_start=False))])"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Now whenever we want a prediction for a house price, this is the system that we will use. For example,\n",
      "    a property that has a floor area of 114 square metres, 3 bedrooms, 2  bathrooms, 2 floors, is a second-hand\n",
      "    development, is semi-detached, has a BER of C1, and is situated in Ballinlough:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "final_model.predict([114, 3, 2, 2, 0, \n",
      "        0, 0, 1, 0, \n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "array([ 232.42375259])"
       ]
      }
     ],
     "prompt_number": 38
    }
   ],
   "metadata": {}
  }
 ]
}