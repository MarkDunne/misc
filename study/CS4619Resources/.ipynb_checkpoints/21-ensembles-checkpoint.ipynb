{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS4619: Artificial Intelligence 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derek Bridge<br> School of Computer Science and Information Technology<br> University College Cork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initialization $\\newcommand{\\Set}[1]{\\{#1\\}}$ $\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ $\\newcommand{\\v}[1]{\\pmb{#1}}$ $\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ $\\newcommand{\\rv}[1]{[#1]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In this lecture, we look at <em>combining</em> models into what is called an <b>ensemble</b>. It turns\n",
    "    out that the ensemble often out-performs the individual models that it contains. There are lots of\n",
    "    analogies here (wisdom of crowds; committees; and so on). You can try using an ensemble whenever you\n",
    "    like, but perhaps their main use is when there is high variance.\n",
    "    Recall that a learner with high variance is quite sensitive to the make-up of its\n",
    "    training set: small changes in the examples on which it is trained can lead to large differences\n",
    "    in the hypothesis that it learns. This can lead to overfitting. \n",
    "</p>\n",
    "<p>\n",
    "    The main ways of building ensembles are: \n",
    "</p>\n",
    "<ul>\n",
    "    <li>bagging</li>\n",
    "    <li>boosting</li>\n",
    "    <li>stacking</li>\n",
    "</ul>\n",
    "<p>\n",
    "    They are general techniques, applicable to both regression and classification.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In <b>bagging</b> (which is a contraction of 'bootstrap aggregating'), several estimators of the\n",
    "    same type make\n",
    "    predictions and the predictions are aggregated. We need to understand two aspects. First, where do the\n",
    "    members of the ensemble come from? Second, how does the ensemble make a prediction?\n",
    "</p>\n",
    "<p>\n",
    "    Suppose we want an ensemble that contains $\\mathit{num}$ estimators. In bagging, we take the training\n",
    "    set and create $\\mathit{num}$ different versions of it by sampling from it with replacement. So now\n",
    "    we have $\\mathit{num}$ training sets, so we can learn $\\mathit{num}$ models:\n",
    "</p>\n",
    "<ul style=\"background-color: lightgray; list-style: none\">\n",
    "    <li>\n",
    "        <b>do</b> $\\mathit{num}$ times\n",
    "        <ul>\n",
    "            <li>sample $|\\mathit{Train}|$ examples with replacement from $\\mathit{Train}$</li>\n",
    "            <li>build a model from this sample</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    Now that we have $\\mathit{num}$ different models, how do we make predictions? \n",
    "<ul>\n",
    "    <li>\n",
    "        For classification, each classifier in the ensemble predicts the class, and these are taken as votes: \n",
    "        the class with the most votes is the one returned as the prediction of the ensemble. (If the\n",
    "        members of the ensemble are capable of outputting probabilities, then you can instead average the\n",
    "        probabilities.)\n",
    "    </li>\n",
    "    <li>\n",
    "        For regression, each regressor in the ensemble predicts the numeric value of the dependent variable, \n",
    "        and these are averaged, and the average is returned as the prediction of the ensemble.\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    In bagging, the aggregation (voting or averaging) is not usually weighted, i.e. all the estimators in\n",
    "    the ensemble are treated equally.\n",
    "</p>\n",
    "<p>\n",
    "    How many models should you use in the ensemble?\n",
    "    To some extent, 'the more, the merrier': predictions typically become more reliable as the ensemble \n",
    "    gets larger. Seldom is the accuracy of the ensemble lower than the accuracy of any of its members\n",
    "    &mdash; but this is not <em>guaranteed</em>. In practice, you can either guess how many models to use,\n",
    "    or do some model selection.\n",
    "</p>\n",
    "<p>\n",
    "    Results are often improved by increasing the diversity of the estimators in the ensemble. \n",
    "    (There are more analogies here: a committee might work better if there is a mixture of expertise.)\n",
    "    One way to\n",
    "    do this is to deliberately use estimators with high variance. \n",
    "</p>\n",
    "<p>\n",
    "    Questions:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        Normally, we want to avoid learners with high variance. Why might we do the opposite in an\n",
    "        ensemble?\n",
    "    </li>\n",
    "    <li>\n",
    "        Suppose the members of the ensemble use the $k$-nearest-neighbours technique. What might you\n",
    "        do to keep variance high?\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    In fact, bagging with $k$-nearest-neighbours may not be successful at all because the different\n",
    "    versions of the training set may not result in sufficiently different members of the ensemble.\n",
    "</p>\n",
    "<p>\n",
    "    To increase diversity, we might combine bagging with some degree of <b>randomization</b>.\n",
    "    There are at least two ways of doing this:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        Modify the learner so that it is partly random (obviously at the expense of some accuracy).\n",
    "        For example, perhaps the learner normally chooses the best of a set of options but you modify it\n",
    "        so that it chooses randomly but with probabilities weighted towards the best options.\n",
    "    </li>\n",
    "    <li>\n",
    "        Another way to randomize is to train on different, randomly-chosen subsets of the features.\n",
    "        This has the advantage of not requiring any changes to the learning algorithm. If you\n",
    "        want to try an ensemble of $k$-nearest-neighbour estimators, then this is probably the best\n",
    "        way of getting some diversity into the ensemble: the different members of the ensemble will\n",
    "        find different neighbours because they will be computing distance using different subsets of\n",
    "        the features.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    scikit-learn does not offer any generic classes for bagging, with or without randomization.\n",
    "    But it does offer <b>random forests</b>, both for classification and regression. These are ensembles\n",
    "    of <b>decision trees</b> (which we have not covered), with randomness built into the learning\n",
    "    algorithm, as per the first bullet point above. Random forests often do very well in Kaggle\n",
    "    competitions. Let's give them a go:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 52440.4152381\n",
      "Linear 1.97927305223e+29\n",
      "Forest 54012.6105452\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"dataset-corkB.csv\")\n",
    "\n",
    "# Remove anomalies from flarea\n",
    "df = df[(df['flarea'].isnull()) | ((df['flarea'] > 10) & (df['flarea'] < 1000))]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# For simplicity, rather than impute missing values, we'll simply remove examples with missing values\n",
    "df.dropna(subset=['flarea', 'bdrms', 'floors', 'devment', 'price'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Encode nominal-valued features\n",
    "# - devment is easy because it is binary-valued\n",
    "df.replace({'devment' : {'SecondHand' : 0, 'New' : 1}}, inplace=True)\n",
    "# - for type we use one-hot encoding\n",
    "one_hot = pd.get_dummies(df['type'], 'type', '_')\n",
    "df.drop('type', axis=1, inplace=True)\n",
    "df = pd.concat([df, one_hot], axis=1)\n",
    "# - similarly for ber\n",
    "one_hot = pd.get_dummies(df['ber'], 'ber', '_')\n",
    "df.drop('ber', axis=1, inplace=True)\n",
    "df = pd.concat([df, one_hot], axis=1)\n",
    "# - and similarly for location\n",
    "one_hot = pd.get_dummies(df['location'], 'location', '_')\n",
    "df.drop('location', axis=1, inplace=True)\n",
    "df = pd.concat([df, one_hot], axis=1)\n",
    "# Scaling - for simplicity, we'll do min-max scaling on just flarea, bdrms and bthrms\n",
    "df['flarea'] -= 40\n",
    "df['flarea'] /= 460\n",
    "df['bdrms'] /= 10\n",
    "df['bthrms'] /= 10\n",
    "\n",
    "X = df[['flarea', 'bdrms', 'bthrms', 'floors', 'devment', \n",
    "        'type_Apartment', 'type_Detached', 'type_Semi-detached', 'type_Terraced', \n",
    "        'ber_B1', 'ber_B2', 'ber_B3', 'ber_C1', 'ber_C2', 'ber_C3', 'ber_D1', 'ber_D2', 'ber_E1', 'ber_E2', 'ber_F', 'ber_G',\n",
    "        'location_Ballinlough', 'location_Ballintemple', 'location_Ballyphehane', 'location_Ballyvolane', \n",
    "        'location_Banduff', 'location_Bishopstown', 'location_Blackpool', 'location_Blackrock', 'location_Carrigrohane', \n",
    "        'location_CityCentre', 'location_Cloghroe', 'location_Donnybrook', 'location_Douglas', 'location_DublinPike', \n",
    "        'location_Farranree', 'location_Fota', 'location_Glanmire', 'location_Glasheen', 'location_Grange', \n",
    "        'location_Gurranabraher', 'location_Inniscarra', 'location_Mayfield', 'location_ModelFarmRoad', \n",
    "        'location_Montenotte', 'location_Ovens', 'location_PassageWest', 'location_Rochestown', 'location_Silversprings', \n",
    "        'location_StLukes', 'location_SundaysWell', 'location_TheLough', 'location_Togher', 'location_TurnersCross', \n",
    "        'location_VictoriaCross', 'location_Waterfall', 'location_WesternRoad', 'location_Wilton']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Train and test\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "estimators = {'Linear': LinearRegression(), 'Tree' : DecisionTreeRegressor(), \n",
    "              'Forest' : RandomForestRegressor(n_estimators = 10)}\n",
    "for estname, est in estimators.items():\n",
    "    mses_test = np.abs(cross_val_score(est, X, y, scoring = 'mean_squared_error', cv = 10))\n",
    "    mean_mse_test = np.mean(mses_test)\n",
    "    print estname, mean_mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <b>Boosting</b> is like bagging in the following respects: \n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        It uses an ensemble of estimators of the same type\n",
    "        (e.g. an ensemble of decision trees, or an ensemble of $k$-nearest-neighbour estimators).\n",
    "    </li>\n",
    "    <li>\n",
    "        Individual predictions are aggregated by voting or averaging. \n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    But boosting differs from bagging in the following ways:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>\n",
    "        In bagging, the models are learned\n",
    "        separately from one another, whereas in boosting each new model is influenced by the accuracy of\n",
    "        the previous one. In this way, boosting tries to learn diverse models &mdash; in particular, ones that\n",
    "        are good at handling the examples that are handled incorrectly by the models already in the ensemble.\n",
    "    </li>\n",
    "    <li>\n",
    "        In bagging, the aggregation typically treats all members of the ensemble equally, whereas in \n",
    "        boosting, aggregation is weighted.\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    The best known boosting method is called <b>AdaBoost</b>. In high-level terms (without formulae\n",
    "    and omitting some details), it\n",
    "    works as follows:\n",
    "</p>\n",
    "<ul style=\"background-color: lightgray; list-style: none\">\n",
    "    <li>\n",
    "        assign equal weight to all examples in $\\mathit{Train}$\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>do</b> $\\mathit{num}$ times\n",
    "        <ul>\n",
    "            <li>build a model from the weighted training set</li>\n",
    "            <li>compute the training error, i.e. the error of this model on the weighted training set</li>\n",
    "            <li>reweight the examples in $\\mathit{Train}$ so that examples the model got wrong get higher weight</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<p>\n",
    "    You can see that subsequent models focus more on examples that earlier models found\n",
    "    'difficult'. The easiest way to reweight is by resampling: sample $\\mathit{Train}$ with replacement\n",
    "    but with probabilities proportional to weight. Hence, more highly-weighted examples will be duplicated\n",
    "    more frequently in the sample.\n",
    "</p>\n",
    "<p>\n",
    "    Now that we have $\\mathit{num}$ different models (and a record of their training error), how do we\n",
    "    make predictions? We use weighted voting or weighted averaging, where the weight is inversely\n",
    "    proportional to the error.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AdaBoost in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    scikit-learn has implementations of AdaBoost for both classification and regression. Compare it to earlier:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 87971.3069619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "est = AdaBoostRegressor(n_estimators = 10)\n",
    "mses_test = np.abs(cross_val_score(est, X, y, scoring = 'mean_squared_error', cv = 10))\n",
    "mean_mse_test = np.mean(mses_test)\n",
    "print 'AdaBoost', mean_mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Boosting can often convert a 'weak' learner into a 'strong' ensemble. It is often more accurate than\n",
    "    bagging. But sometimes it overfits, resulting in an estimator that is less accurate than a single\n",
    "    estimator built from the same data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In contrast to bagging and boosting, <b>stacking</b> combines different types of model, e.g. \n",
    "    linear regression with\n",
    "    $k$-nearest-neighbours regression. We train them on the same training set. To make predictions, we run\n",
    "    each estimator and aggregate: voting or averaging as appropriate.\n",
    "</p>\n",
    "<p>\n",
    "    But this ignores the fact that some members of the ensemble may perform better than others, and these should\n",
    "    be given greater weight in the aggregation. In stacking, we use a <b>meta-learner</b> to learn the\n",
    "    weights. This is still supervised learning so we need a training set so that it knows the correct answers.\n",
    "</p>\n",
    "<p>\n",
    "    In high-level terms, this is how stacking works:\n",
    "</p>\n",
    "<ul style=\"background-color: lightgray; list-style: none\">\n",
    "    <li>\n",
    "        split $\\mathit{Train}$ into two: $\\mathit{Train}_1$ and $\\mathit{Train}_2$\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>do</b> $\\mathit{num}$ times:\n",
    "        <ul>\n",
    "            <li>\n",
    "                build a model from $\\mathit{Train}_1$ (a different model each time)\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        create a training set for the meta-learner as follows:<br />\n",
    "        <b>for each</b> $\\Tuple{\\v{x}, y} \\in \\mathit{Train}_2$:\n",
    "        <ul>\n",
    "            <li>there are $\\mathit{num}$ features, which are the predictions for $\\v{x}$ made by the\n",
    "                $\\mathit{num}$ models \n",
    "            </li>\n",
    "            <li>the dependent variable is $y$ (from $\\mathit{Train}_2$)\n",
    "            </li>\n",
    "        </ul>\n",
    "     </li>\n",
    "     <li>\n",
    "         build a model for this new training set\n",
    "     </li>\n",
    "</ul>\n",
    "<p>\n",
    "    You can make this more robust by using $k$-fold cross-validation instead of a single split into\n",
    "    $\\mathit{Train}_1$ and $\\mathit{Train}_2$.\n",
    "</p>\n",
    "<p>\n",
    "    The meta-learner is often something simple, e.g. linear or logistic regression.\n",
    "</p>\n",
    "<p>\n",
    "    Stacking is less well-understood than bagging and boosting. While it can work well, it \n",
    "    often doesn't!\n",
    "</p>\n",
    "<p>\n",
    "    I'm not aware of a scikit-learn implementation, so we can't try it without writing it for\n",
    "    ourselves.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
