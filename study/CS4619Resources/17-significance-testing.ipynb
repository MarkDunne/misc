{
 "metadata": {
  "name": "",
  "signature": "sha256:2dfd290d53fcbe827049371442014af435eb659ca0a31f20424f3e4af7d8b30e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Significance Testing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Significance Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    How do we compare two regressors or classifiers? It seems simple: perform error estimation (using a suitable performance\n",
      "    measure and a suitable method, such as repeated holdout) and choose the one that has the lower error or higher accuracy.\n",
      "    However, it may be that the difference in their error or accuracy is due to chance. Sometimes it is important to establish\n",
      "    whether one system is <em>really</em> better than another. This is the subject of what statisticians call 'hypothesis\n",
      "    testing' (although this uses the word 'hypothesis' in a different way from how we have used it so far\n",
      "    and so I shall avoid their use of that word in the rest of this lecture). It is a vast\n",
      "    and complicated subject. We will skim the surface, looking at some simple techniques that get used in data mining/machine\n",
      "    learning.\n",
      "</p>\n",
      "<p>\n",
      "    Trivia: We are looking at Student's $t$-tests, which are credited to William Sealy Gosset, who worked for Guiness in\n",
      "    Dublin. His statistical work had the goal of making the production of Guiness more consistent. He was not allowed to\n",
      "    publish his ideas. But he did so anyway, under the pseudonym of <i>Student</i>. This work was published in\n",
      "    1908 in <i>Biometrika</i>.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Paired $t$-Test"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " <p>\n",
      "    The $t$-test is used to find out whether the difference between two means is\n",
      "    significant &mdash; to quantify the extent to which the difference may be due to chance. In our case, the two means\n",
      "    come from the performances of two estimators (regressors or classifiers).\n",
      "</p>\n",
      "<p>\n",
      "    We're assuming two estimators being tested in a single domain (as opposed to comparing them across\n",
      "    several domains). For each estimator, we have run several trials and recorded some performance measure\n",
      "    in each trial (e.g. mean squared error for a regressor or classification accuracy for a classifier).\n",
      "    For generality, let's refer to the performance measure as $pm$.\n",
      "    The trials might be the iterations of repeated holdout, or the iterations of $k$-fold cross-validation,\n",
      "    for example. For generality, let's refer to the number of trials as $num\\_trials$.\n",
      "</p>\n",
      "<p>\n",
      "    We assume that, in each trial, both estimators are trained on the same training set and both are tested on the\n",
      "    same test set. This is why we can use a <em>paired</em> $t$-test. We would need to use a different significance\n",
      "    testing method in the case where in a trial the training and test set for one estimator was not necessarily the same\n",
      "    as the training and test set for the other estimator.\n",
      "</p>\n",
      "<p>\n",
      "    So we have two samples:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>$\\rv{pm_1(L_1), ..., pm_{num\\_trials}(L_1)}$</li>\n",
      "    <li>$\\rv{pm_1(L_2), ..., pm_{num\\_trials}(L_2)}$</li>\n",
      "</ul>\n",
      "<p>\n",
      "    The paired $t$-test looks at the means and standard deviations of the differences between the two samples. \n",
      "    So we create a sample of differences \n",
      "    $$\\v{d} = \\Set{\\v{d}_1, ..., \\v{d}_{num\\_trials}}$$ \n",
      "    where $\\v{d}_i = pm_i(L_1) - pm_i(L_2)$. In other words, $\\v{d}$ contains\n",
      "    $$\\Set{pm_1(L_1) - pm_1(L_2), ..., pm_{num\\_trials}(L_1) - pm_{num\\_trials}(L_2)}$$\n",
      "</p>\n",
      "<p>\n",
      "    We compute the mean of this set of differences, which we will write $\\bar{d}$:\n",
      "    $$\\bar{\\v{d}} = \\frac{\\sum_{i=1}^{num\\_trials} \\v{d}_i}{num\\_trials}$$\n",
      "    and its standard deviation:\n",
      "    $$\\sigma_d = \\sqrt{\\frac{\\sum_{i=1}^{num\\_trials}(\\v{d}_i - \\bar{\\v{d}})^2}{num\\_trials - 1}}$$\n",
      "</p>\n",
      "<p>\n",
      "    We then compute the <b>$t$-value</b>:\n",
      "    $$t = \\frac{\\bar{\\v{d}}}{\\frac{\\sigma_d}{\\sqrt{num\\_trials}}}$$\n",
      "    $t$ increases as the difference between the means of two samples increases. It is not bounded to a fixed range such as $[0, 1]$\n",
      "    and can grow arbitrarily large, which has some consequences below.\n",
      "</p>\n",
      "<p>\n",
      "    The $t$-value can be converted into a probability, called a <b>$p$-value</b>; obviously, this does lie in $[0, 1]$.\n",
      "    The $p$-value is the probability of the difference in the performances of the two learners being due to chance.\n",
      "    Large $t$-values have small $p$-values.\n",
      "</p>\n",
      "<p>\n",
      "    We choose a significance level (e.g. 0.05 or 0.01).\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        If the $p$-value is lower than this amount, then we conclude that the difference is not due to chance (at that significance\n",
      "        level). \n",
      "    </li>\n",
      "    <li>\n",
      "        If the $p$-value is greater than or equal to this amount, then we conclude that we cannot rule out the possibility\n",
      "        that the difference is due to chance. \n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    (For those who like to be precise, note the difference in wording in the above bullet points.)\n",
      "</p>\n",
      "<p>\n",
      "    Let's put some detail on the above. \n",
      "</p>\n",
      "<p>\n",
      "    In days gone by, you would consult a $t$-table. It's important to know whether\n",
      "    your $t$-table is for one-tailed tests or two-tailed tests. The difference will be explained later. Here we are doing a two-tailed\n",
      "    test and the fragment shown below is for two-tailed tests:\n",
      "</p>\n",
      "<table>\n",
      "    <tr>\n",
      "        <th colspan=\"2\" rowspan=\"2\">Two-tailed</th><th colspan=\"5\">$p$</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>0.2</th><th>0.1</th><th>0.05</th><th>0.02</th><th>0.01</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th rowspan=\"3\">df</th><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>9</th><td>1.383</td><td>1.833</td><td>2.262</td><td>2.821</td><td>3.25</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td>\n",
      "    </tr>\n",
      "</table>\n",
      "<ul>\n",
      "    <li>\n",
      "        The columns are headed by <b>significance levels</b>. \n",
      "    </li>\n",
      "    <li>\n",
      "        The rows are headed by <b>degrees of freedom</b>. Your degree of freedom is $num\\_trials - 1$.\n",
      "    </li>\n",
      "    <li>\n",
      "        The cells of the table contain <b>critical $t$-values</b>. \n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    So what you do is the following:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        Find the critical $t$-value in the cell of the table for your\n",
      "        desired significance level and for your degree of freedom. \n",
      "    </li>\n",
      "    <li>\n",
      "        Ignoring its sign, if the $t$-value that you computed \n",
      "        is <em>greater than or equal to</em> the critical \n",
      "        $t$-value in the table, then you can conclude that your $p$-value is less than the chosen significance level and hence\n",
      "        the difference in performance is not due to chance (at that significance level).\n",
      "        <p>\n",
      "        E.g. if $num\\_trials = 10$ and you are interested in significance at the 0.1 level, you compare your $t$-value with\n",
      "        1.833 (the critical $t$-value from the table).\n",
      "        </p>\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Nowadays, of course, we use software:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        The software takes in your samples and usually gives you your $p$-value directly. Again it is important to know\n",
      "        whether it is giving you $p$-values for one-tailed or two-tailed tests.\n",
      "    </li>\n",
      "    <li>\n",
      "        You compare the $p$-value with your desired significance level. If the $p$-value is <em>less than</em> the significance\n",
      "        level, then you can conclude that the difference in performance is not due to chance (at that significance level).\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We'll run repeated holdout with $num\\_trials = 10$ on the CS1109 dataset for kNN (with $k$ arbitrarily\n",
      "    set to 3 and unweighted) and for logistic regression:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "def repeated_holdout_for_classification(estimator, X, y, num_iterations = 10, test_size = 0.3):\n",
      "    accuracies_test = np.zeros(num_iterations)\n",
      "    sss = StratifiedShuffleSplit(y, n_iter = num_iterations, test_size = test_size,\n",
      "                                random_state = np.random)\n",
      "    for i, (train_indexes, test_indexes) in zip(range(num_iterations), sss):\n",
      "        X_train = X[train_indexes]\n",
      "        y_train = y[train_indexes]\n",
      "        X_test = X[test_indexes]\n",
      "        y_test = y[test_indexes]\n",
      "        estimator.fit(X_train, y_train)\n",
      "        y_predicted = estimator.predict(X_test)\n",
      "        accuracies_test[i] = accuracy_score(y_test, y_predicted)\n",
      "    return accuracies_test\n",
      "\n",
      "# Read CSV file\n",
      "df = pd.read_csv(\"dataset-cs1109.csv\")\n",
      "\n",
      "# Get the feature-values into a separate numpy arrays of numbers and the target values into a separate \n",
      "# numpy arrays of ints\n",
      "X = df[['lect', 'lab', 'cao']].values\n",
      "y = df['outcome'].values\n",
      "\n",
      "lr = LogisticRegression()\n",
      "accuracies_lr = repeated_holdout_for_classification(lr, X, y, num_iterations = 10, test_size = 0.3)\n",
      "knn = KNeighborsClassifier(n_neighbors = 3)\n",
      "accuracies_knn = repeated_holdout_for_classification(knn, X, y, num_iterations = 10, test_size = 0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    If you are going to use a $t$-table, then you must compute your $t$-value:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = accuracies_lr - accuracies_knn\n",
      "t = np.mean(d) / (np.std(d, ddof = 1) / math.sqrt(d.size))\n",
      "print t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3.25105798017\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Suppose we are interested in siginficance at the 0.1 level. We find the cell of the table for $p = 0.1$ and $df = 9$, which is\n",
      "    1.833. Ignoring the sign of our $t$-value, since it is greater than the critical $t$-value (3.25 > 1.833), we conclude\n",
      "    that the difference in performance is not due to chance (at the 0.1 significance level). Logistic regression is\n",
      "    better than kNN ($p = 0.1$).\n",
      "</p>\n",
      "<p>\n",
      "    As mentioned, we don't need to compute $t$-values, nor look up the critical $t$-value in a table. We can just feed the sets of\n",
      "    of $pm$ values into a function from a Python stats package and get it to do all the work:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.mstats import ttest_rel\n",
      "\n",
      "t, two_tailed_prob = ttest_rel(accuracies_lr.tolist(), accuracies_knn.tolist())\n",
      "print two_tailed_prob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.00998046126927\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Since 0.00998 is less than the desired significance level of 0.1, as above we conclude that the difference in\n",
      "    performance is not due to chance (at the 0.1 significance level).\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "One-Tailed and Two-Tailed Tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Suppose you are comparing estimator $L_1$ and estimator $L_2$. We are using a paired $t$-test to try to reject the possibility\n",
      "    that any difference in their performance (i.e. the difference in the means of the two samples of their performances)\n",
      "    is due to chance. We can reject this in two\n",
      "    ways: first, if the difference is significantly less than zero ($L_2$ better than $L_1$), and second if the difference is \n",
      "    significantly greater than zero ($L_1$ better than $L_2$). This is a <b>two-tailed test</b>, and it is what we explained\n",
      "    and exemplified above.\n",
      "</p>\n",
      "<p>\n",
      "    But suppose instead we were concerned only to test whether $L_2$ was better than $L_1$. This might be the case if we know\n",
      "    or strongly believe that $L_2$ cannot be worse than $L_1$ &mdash; perhaps $L_1$ is a baseline estimator and $L_2$ is \n",
      "    almost certain to be an improvement on $L_1$. In this case, we can use a <b>one-tailed test</b>. It considers only one\n",
      "    way of rejecting the assumption that the performance of the two estimators is the same.\n",
      "</p>\n",
      "<p>\n",
      "    A one-tailed test is carried out exactly as described for two-tailed tests. If you are using the old-fashioned method (table\n",
      "    look-up), then you need a $t$-table for one-tailed tests, e.g.:\n",
      "</p>\n",
      " <table>\n",
      "    <tr>\n",
      "        <th colspan=\"2\" rowspan=\"2\">One-tailed</th><th colspan=\"5\">$p$</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>0.1</th><th>0.05</th><th>0.025</th><th>0.01</th><th>0.005</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th rowspan=\"3\">df</th><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>9</th><td>1.383</td><td>1.833</td><td>2.262</td><td>2.821</td><td>3.25</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td>\n",
      "    </tr>\n",
      "</table>\n",
      "<p>\n",
      "    For the same significance level, a smaller $t$-value now suffices. E.g. at the 0.1 level, your $t$-value needs to be\n",
      "    greater than or equal to 1.383 (whereas, for a two-tailed test, it needed to be greater than or equal to 1.833).\n",
      "</p>\n",
      "<p>\n",
      "    If using software to compute $p$-values, you halve the two-tailed $p$-value and compare that with your desired\n",
      "    significance level. For example, if the software outputs a two-tailed $p$-value of 0.017, you check wether $0.017/2 = 0.0085$\n",
      "    is lower than your desired significance level.\n",
      "</p>\n",
      "<p>\n",
      "    You can see that one-tailed tests are less stringent. It would be wrong to use them for the sole purpose of achieving\n",
      "    significance! The risks of using them inappropriately are that you may miss a significant effect in the opposite direction.\n",
      "    So you use them when the chances and consequences of an effect in the opposite direction are negligible.\n",
      "</p>\n",
      "<p>\n",
      "    For example, we can test whether the difference in performance between Logistic Regression and the majority-class classifier\n",
      "    is statistically significant using a one-tailed test:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.dummy import DummyClassifier\n",
      "maj = DummyClassifier(strategy = \"most_frequent\")\n",
      "accuracies_maj = repeated_holdout_for_classification(maj, X, y, num_iterations = 10, test_size = 0.3)\n",
      "\n",
      "t, two_tailed_prob = ttest_rel(accuracies_lr.tolist(), accuracies_maj.tolist())\n",
      "print two_tailed_prob / 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.69058479801e-09\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Since 0.00000000169 is less than 0.1, we can conclude that the difference in their performance is not due to chance (at the 0.1\n",
      "    significance level).\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Assumptions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The $t$-test is a parametric test, which means that it makes some assumptions about the samples. If these assumptions are not true, \n",
      "    the test can give misleading results. Here are the assumptions:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        The $t$-test requires that the samples come from normally distributed populations. Simulations have shown that \n",
      "        it is usually sufficient to ensure that the\n",
      "        test sets contain more than 30 examples. So, for example, if you are using 10-fold cross validation, \n",
      "        then you need a dataset with 300 examples. (The CS1109 dataset has 342 examples, so we're OK!)\n",
      "    </li>\n",
      "    <li>\n",
      "        The $t$-test assumes that the samples are representative. This makes it essential that the dataset is representative and\n",
      "        that you are using a proper method for error estimation.\n",
      "    </li>\n",
      "    <li>\n",
      "        The paired $t$-test assumes that the two samples have equal variance. You could draw boxplots and visually check for\n",
      "        similar height boxes.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Be warned too that there are some concerns about using this test on resampled data: overlap in the training sets biases \n",
      "    the classifiers. There is a vast literature of variations that try to correct for this.\n",
      "</p>\n",
      "<p>\n",
      "    Be warned too that there is a 'gotcha'. Sometimes you can arbitrarily increase $t$ by running more and more trials.\n",
      "    But this is an improper way of achieving 'significance'! (Try it: increase num_iterations above.)\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "McNemar's Test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    McNemar's test is a non-parametric test &mdash; in other words, it does not make any assumptions about the distribution of the\n",
      "    performance measures. But it is only for use with <em>classifiers</em>. We will show it for classification accuracy.\n",
      "</p>\n",
      "<p>\n",
      "    In this test, you compute what is called a <b>contingency matrx</b> (not to be confused with the...erm...confusion matrix):\n",
      "</p>\n",
      "<table>\n",
      "    <tr>\n",
      "        <th colspan=\"2\" rowspan=\"2\"><th colspan=\"2\">$L_2$</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>Wrong</th><th>Correct</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th rowspan=\"2\">$L_1$</th><th>Wrong</th><td>$c_{ww}$</td><td>$c_{wc}$</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <th>Correct</th><td>$c_{cw}$</td><td>$c_{cc}$</td>\n",
      "    </tr>\n",
      "</table>\n",
      "<ul>\n",
      "    <li>\n",
      "        $c_{ww}$ is a count of test examples that both $L_1$ and $L_2$ misclassified\n",
      "    </li>\n",
      "    <li>\n",
      "        $c_{wc}$ is a count of test examples that $L_1$ misclassified but $L_2$ correctly classified.\n",
      "    </li>\n",
      "    <li>\n",
      "        $c_{cw}$ is a count of test examples that $L_1$ correctly classified but $L_2$ misclassified.\n",
      "    </li>\n",
      "    <li>\n",
      "        $c_{cc}$ is a count of test examples that both $L_1$ and $L_2$ correctly classified.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Then you compute the following chi$^2$-value:\n",
      "    $$\\chi^2 = \\frac{(abs(c_{wc} - c_{cw}) - 1)^2}{c_{wc} + c_{cw}}$$\n",
      "</p>\n",
      "<p>\n",
      "    You look up your desired level of significance and your degree of freedom in a $\\chi^2$-table. If the value\n",
      "    you computed is greater than or equal to the value in the table, you conclude that the difference in performance\n",
      "    is not due to chance (at that significance level). Alternatively, you use software to compute a $p$-value, which you\n",
      "    then compare with your desired significance level.\n",
      "</p>\n",
      "<p>\n",
      "    There's no straightforward Python implementation. Here's my own\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import chi2\n",
      "\n",
      "def contingency_matrix(y_test, y_predicted_l1, y_predicted_l2):\n",
      "    cww = ((y_test != y_predicted_l1) & (y_test != y_predicted_l2)).sum()\n",
      "    cwc = ((y_test != y_predicted_l1) & (y_test == y_predicted_l2)).sum()\n",
      "    ccw = ((y_test == y_predicted_l1) & (y_test != y_predicted_l2)).sum()\n",
      "    ccc = ((y_test == y_predicted_l1) & (y_test == y_predicted_l2)).sum()\n",
      "    return np.array([[cww, cwc], [ccw, ccc]])\n",
      "\n",
      "def mcnemar(cm):\n",
      "    if cm[0,1] + cm[1, 0] < 20:\n",
      "        print \"Warning: Possibly misleading result!\"\n",
      "    chi2_value = (abs(cm[0, 1] - cm[1, 0]) - 1)**2 * 1.0 / (cm[0, 1] + cm[1, 0])\n",
      "    p_value = 1 - chi2.cdf(chi2_value, 1)\n",
      "    return p_value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Here we'll run it using just holdout:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = np.random)\n",
      "l1 = LogisticRegression()\n",
      "l1.fit(X_train, y_train)\n",
      "y_predicted_l1 = l1.predict(X_test)\n",
      "l2 = KNeighborsClassifier(n_neighbors = 3)\n",
      "l2.fit(X_train, y_train)\n",
      "y_predicted_l2 = l2.predict(X_test)\n",
      "cm = contingency_matrix(y_test, y_predicted_l1, y_predicted_l2)\n",
      "print cm\n",
      "two_tailed_pvalue = mcnemar(cm)\n",
      "print two_tailed_pvalue"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[26  8]\n",
        " [ 2 67]]\n",
        "Warning: Possibly misleading result!\n",
        "0.113846298007\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Although this is a non-parametric test, if $c_{wc} + c_{cw} < 20$, then using $\\chi^2$-tables gives misleading\n",
      "    results. \n",
      "</p>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}