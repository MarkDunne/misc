{
 "metadata": {
  "name": "",
  "signature": "sha256:57a3185338dd5f83d7e0a3eb1235d796d8352a9b86eb2c908b61491165ab8568"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Nearest-Neighbours for Regression"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "k-Nearest Neighbours for Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "   Linear Regression is an example of <b>parametric learning</b>: it 'chooses' from a <em>restricted</em> set of hypotheses\n",
      "   (models) by finding parameters that give a good fit to an unrestricted training set. This is often effective. But we\n",
      "   might suspect that we could learn a better model if we had an unrestricted set of hypotheses.\n",
      "</p>\n",
      "<p>\n",
      "   In this lecture, we look at <b>nearest-neighbour models</b> for regression. These give us simple examples of \n",
      "   <b>nonparametric learning</b>, where there is no restriction on the models: the more complicated the data, the more\n",
      "   complicated the hypotheses.\n",
      "</p>\n",
      "<p>\n",
      "    The key assumption of nearest-neighbour models for regression is that $\\v{x}$'s value for $y$ will be similar to \n",
      "    $\\v{x}$'s neighbours' values for $y$. If this assumption holds, then to predict $\\v{x}$'s value for $y$, we find \n",
      "    $\\v{x}$'s neighbours and base our prediction on their value for $y$. \n",
      "</p>\n",
      "<p>\n",
      "    How many neighbours should we use? The simplest method is to fix a value, designated $k$. This means that $\\v{x}$'s\n",
      "    value for $y$ is obtained from the $y$-values of the $k$ nearest neighbours of $\\v{x}$. Hence, these algorithms\n",
      "    are often referred to as <b>$k$-nearest-neighbours</b> methods, abbreviated to <b>kNN</b>. We will look first at\n",
      "    the case where $k = 1$, and then at the case where $k > 1$. But first we consider how to measure the distance\n",
      "    between two examples.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Let $\\v{x}$ be one vector of feature values and $\\v{x}'$ be another. Simplest is to measure their Euclidean distance:\n",
      "    $$d(\\v{x}, \\v{x}') = \\sqrt{(\\v{x}_1 - \\v{x}_1')^2 + (\\v{x}_2 - \\v{x}_2')^2 + \\ldots + (\\v{x}_n - \\v{x}_n')^2}$$\n",
      "    or, more concisely:\n",
      "    $$d(\\v{x}, \\v{x}') = \\sqrt{\\sum_{j=1}^n(\\v{x}_j - \\v{x}_j')^2}$$\n",
      "    It has a nice vectorized implementation in numpy Python:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dist(x, xprime):\n",
      "    return np.sqrt(np.sum((x - xprime)**2))\n",
      "\n",
      "# Example\n",
      "my_house = np.array([107, 3, 2])\n",
      "your_house = np.array([120, 3, 1])\n",
      "dist(my_house, your_house)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "13.038404810405298"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Thoughts About Measuring Distances"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Questions:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        (Easy) For finding nearest neighbours, we don't really need to take the square root. Why not?\n",
      "    </li>\n",
      "    <li>\n",
      "        (Difficult) Does the example bring to mind a problem with using definitions of distance \n",
      "        like this one on features like these?\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    In passing, let's mention that there are other ways to define distance. In the scikit-learn documentation, you\n",
      "    might see Minkowski distance being mentioned. It is a generalization of Euclidean distance:\n",
      "    $$d(\\v{x}, \\v{x}') = (\\sum_{j=1}^n(|\\v{x}_j - \\v{x}_j'|)^p)^{\\frac{1}{p}}$$\n",
      "    When $p = 1$, we get Manhattan distance; when $p = 2$, we get Euclidean distance. \n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "1NN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Suppose we have a training set $\\v{X}$. We want to predict $\\hat{y}$ for feature vector $\\v{x}$. For each $\\v{x}^{(i)} \\in\n",
      "    \\v{X}$, we compute $d(\\v{x}, \\v{x}^{(i)})$. We find the $i$ such that $\\v{x}^{(i)} \\in \\v{X}$ has the smallest distance to\n",
      "    $\\v{x}$. Then, our prediction is $\\v{y}^{(i)}$.\n",
      "</p>\n",
      "<p>\n",
      "    A minor complication is what to do when there is a tie, i.e. two or more examples share the same smallest distance to\n",
      "    $\\v{x}$. In this case, we might just take the first one that we found (which is what the Python does, below) or we might\n",
      "    choose one of them at random. \n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "My Python Implementation of 1NN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    First, let's read in the CorkA property dataset:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['flarea', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Here's a function that implements 1NN:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def NN_predict(x):\n",
      "    dists = [dist(x, xi) for xi in X]\n",
      "    index = np.argmin(dists)\n",
      "    return y[index]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Let's make a 1NN prediction for $\\cv{114\\\\3\\\\2}$. It turns out that its nearest neighbour in the CorkA property \n",
      "    dataset is $\\Tuple{\\cv{115.2\\\\4\\\\2\\\\}, 385}$, so we predict a selling price of 385:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example prediction\n",
      "NN_predict(np.array([114, 3, 2]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "385"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "scikit-learn Implementation of 1NN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    As usual, we don't really need to write it for ourselves:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "\n",
      "estimator = KNeighborsRegressor(n_neighbors = 1)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Example prediction\n",
      "estimator.predict([[114, 3, 2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([ 385.])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Questions:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        What do you think the fit method is doing? Or, to put this another way, where is all the work taking place?\n",
      "    </li>\n",
      "    <li>\n",
      "        You can change which distance function it uses. The defaults are p=2, metric='minkowski', i.e. Euclidean. \n",
      "        How would you edit the code above to use Manhattan distance?\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "kNN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p> \n",
      "    A problem arises with 1NN if the training set contains noise. If the values of the features or dependent variable are\n",
      "    incorrect, they will lead to incorrect predictions. \n",
      "    This motivates use of $k > 1$.\n",
      "</p>\n",
      "<p>\n",
      "    In kNN in general, we find the $k$ examples in the training set whose distance from $\\v{x}$ is smallest and we base our\n",
      "    prediction on their $y$-values. Typical values for $k$ might be 3, 5, 10 or 20, although there may be scenarios where\n",
      "    much larger values (100, say) would be used. With $k > 1$, the impact of noisy examples in the training set\n",
      "    is reduced. If $k$ is 'too small', noisy examples may still have too great an impact. If $k$ is 'too big', you are\n",
      "    no longer exploiting any structure in the training data (although we will see below that distance-weighting can help\n",
      "    reduce this problem).\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Parameters versus Hyperparameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    $k$ is a kind of parameter, like $\\v{\\beta}$ in linear regression.\n",
      "    But there's a big difference: it's the job of your linear regression learning algorithm to come up with \n",
      "    the value of $\\v{\\beta}$, whereas the value of $k$ is not set by the kNN algorithm &mdash; its value is\n",
      "    fed into the algorithm. Let's make this distinction:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        <b>Parameters</b>: variables whose values are set by the learning algorithm &mdash; such as $\\v{\\beta}$\n",
      "        in linear regression.\n",
      "    </li>\n",
      "    <li>\n",
      "        <b>Hyperparameters</b>: variables whose values are not set by the learning algorithm, rather they\n",
      "        are supplied to the learning algorithm &mdash; such as $k$ in kNN.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Question:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        Give an example of a hyperparameter that we saw when studying linear regression.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    If the learning algorithm does not set them, how will we choose the values for hyperparameters?\n",
      "    Specifically, how do we choose the value of $k$? \n",
      "</p>\n",
      "<p>\n",
      "    Often, we'll just make a guess &mdash; hopefully, an informed guess. E.g. we know that a value for\n",
      "    $k$ between 3 and 20 has often worked well on other datasets. In an up-coming lecture, we will\n",
      "    re-visit this topic and see that there are also automatic methods that can help us to choose the\n",
      "    values of hyperparameters.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Unweighted kNN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The simplest approach is to report the mean of the neighbours' $y$-values:\n",
      "    $$\\hat{y} = \\frac{\\sum_{\\v{x}^{(i)} \\in NN}\\v{y}^{(i)}}{k}$$\n",
      "    where $NN$ is the set of $\\v{x}$'s $k$ nearest-neighbours.\n",
      "</p>\n",
      "<p>\n",
      "    As before, if there are ties for $k$th place, this code just takes the ones that the sorting places first in the ordering:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kNN_predict(x, k):\n",
      "    dists = [dist(x, xi) for xi in X]\n",
      "    indexes = np.argsort(dists)[:k]\n",
      "    return np.mean(y[indexes])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    If we use 3NN to predict for $\\v{x} = \\cv{114\\\\3\\\\2}$ again, we find the three nearest neighbours are\n",
      "    $\\Tuple{\\cv{115.2\\\\4\\\\2}, 385}, \\Tuple{\\cv{112.4\\\\3\\\\2}, 225}, \\Tuple{\\cv{112.4\\\\3\\\\2}, 225}$\n",
      "    and so we predict the mean, $\\hat{y} = \\frac{385 + 225 + 225}{3} = 278.33$:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example prediction\n",
      "kNN_predict(np.array([114, 3, 2]), k = 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "278.33333333333331"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    A better implementation would avoid the sorting, which could become expensive for a large training set. Instead, it\n",
      "    would insert the distances into a priority-ordered queue whose length was constrained to be $k$. \n",
      "</p>\n",
      "<p>\n",
      "    But the best thing is to use scikit-learn:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = KNeighborsRegressor(n_neighbors = 3)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Example prediction\n",
      "estimator.predict([[114, 3, 2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([ 278.33333333])"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Distance-weighted kNN"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In unweighted kNN, each neighbour contributes equally to the prediction. An obvious variant is to give greater weight\n",
      "    to closer neighbours: the closer you are, the more you contribute to the computation of the mean. We need a weighted mean:\n",
      "    $$\\hat{y} = \\frac{\\sum_{\\v{x}^{(i)} \\in NN}(\\v{w}^{(i)} \\times \\v{y}^{(i)})}{\\sum_{\\v{x}^{(i)} \\in NN}\\v{w}^{(i)}}$$\n",
      "</p>\n",
      "<p>\n",
      "    The weights $\\v{w}^{(i)}$ should be inversely proportional to the distances, so we could use the following:\n",
      "    $$\\v{w}^{(i)} = \\frac{1}{dist(\\v{x}, \\v{x}^{(i)})}$$\n",
      "</p>\n",
      "<p>\n",
      "    Let's use distance-weighted 3NN to predict for $\\v{x} = \\cv{114\\\\3\\\\2}$. We get the same \n",
      "    neighbours as above. But we need their distances from $\\v{x}$, which, to two decimal places, are 1.56, 1.6 and 1.6,\n",
      "    respectively.\n",
      "    The weights are $1/1.56 = 0.641$, $1/1.6 = 0.625$ and \n",
      "    $1/1.6 = 0.625$.\n",
      "    And the prediction is the weighted average:\n",
      "    $$\\hat{y} = \\frac{0.641 \\times 385 + 0.625 \\times 225 + 0.625 \\times 225}{0.641 + 0.625 + 0.625} = 279.2$$\n",
      "<p>\n",
      "    We can easily do this in scikit-learn by specifying weights='distance':\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = KNeighborsRegressor(n_neighbors = 3, weights = 'distance')\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Example prediction\n",
      "estimator.predict([[114, 3, 2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "array([ 279.19021761])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Fixing a Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We defined the weights $\\v{w}^{(i)}$ to be inversely proportional to the distances:\n",
      "    $$\\v{w}^{(i)} = \\frac{1}{dist(\\v{x}, \\v{x}^{(i)})}$$\n",
      "</p>\n",
      "<p>\n",
      "    But this gives the possibility of division by zero: it will arise if there is ever an example in the training set whose\n",
      "    distance from $\\v{x}$ is zero (presumably, exact matches). One solution to this is to adjust the denominator:\n",
      "    $$\\v{w}^{(i)} = \\frac{1}{0.001 + dist(\\v{x}, \\v{x}^{(i)})}$$\n",
      "</p>\n",
      "<p>\n",
      "    Somewhat unbelievably, scikit-learn has not dealt gracefully with this problem:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example prediction\n",
      "estimator.predict([[101.9, 3, 3]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([ nan])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Instead, we can supply our own function that takes in an array of distances and returns an array of weights. \n",
      "    My inv_distances function does this in a vectorized way:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def inv_distances(dists):\n",
      "    return 1 / (0.0001 + dists)\n",
      "\n",
      "estimator = KNeighborsRegressor(n_neighbors = 3, weights = inv_distances)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Example prediction\n",
      "estimator.predict([[114, 3, 2], [101.9, 3, 3]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([ 279.1901632 ,  180.04828505])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Shepard's Method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    If you're using distance-weighting then, instead of basing the prediction $\\hat{y}$ on just the $k$ nearest neighbours\n",
      "    of $\\v{x}$, you could base it on <em>all</em> the examples in the training set. In other words, use $k = m$.\n",
      "    Distant examples will have little effect on the prediction. The disadvantage is that it will run more slowly.\n",
      "</p>\n",
      "<p>\n",
      "    Let's try it:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m, n = X.shape\n",
      "\n",
      "estimator = KNeighborsRegressor(n_neighbors = m, weights = inv_distances)\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Example prediction\n",
      "estimator.predict([[114, 3, 2], [101.9, 3, 3]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "array([ 249.12766093,  180.12020134])"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Advantages of Nearest-Neighbours Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ul>\n",
      "    <li>\n",
      "        Instead of fitting a <em>global</em> model to the training set, nearest neighbour methods estimate the target function \n",
      "        in a <em>local</em> fashion:\n",
      "        they construct a different approximation to the target function for each $\\v{x}$ for which a prediction is\n",
      "        sought. This is advantageous when the target function is complex. Other methods construct a single hypothesis \n",
      "        that has to perform well across the whole space of examples. This makes kNN (especially with $k > 1$\n",
      "        and distance weighted) a very effective prediction method for many practical problems.\n",
      "    </li>\n",
      "    <li>\n",
      "        Most learning algorithms only work with features whose values are numeric. kNN can work on\n",
      "        features whose values are not numeric &mdash; provided a suitable distance function can be defined.\n",
      "    </li>\n",
      "    <li>\n",
      "        It is easy for kNN to incrementally incorporate new training examples: if new training examples become available,\n",
      "        you simply add them to the training set &mdash; that's it! By contrast, many other learning algorithms would have \n",
      "        to re-compute their model from scratch.\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Disadvantages of Nearest-Neighbours Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ul>\n",
      "    <li>\n",
      "        The cost of kNN prediction can be high.\n",
      "    </li>\n",
      "    <li>\n",
      "        KNN methods often suffer from the <b>curse of dimensionality</b>.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    We look at both these disadvantages &mdash; and ways of overcoming them &mdash; in the next two subsections.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Cost of kNN Prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In kNN prediction, pretty much all the work takes place on-demand, at prediction time. In the way we have\n",
      "    presented it so far, every time it makes a prediction, the kNN algorithm must compute the distance between\n",
      "    $\\v{x}$ and <em>every</em> $\\v{x}^{(i)}$ in the training set. For\n",
      "    a time, it was thought that this confined kNN methods to datasets with few training examples (small $m$).\n",
      "</p>\n",
      "<p>\n",
      "    But there are solutions that can allow kNN to be used more widely: to use clever data structures that\n",
      "    make it possible to find the nearest neighbours without computing the distance to every training\n",
      "    example.\n",
      "</p>\n",
      "<p>\n",
      "    One such data structure is the $kd$-tree. The training examples are stored in the leaves of a tree.\n",
      "    The storage policy ensures that examples that are close to each other (according to the distance function)\n",
      "    are stored either at the same leaf or at nearby leaves. Interior nodes of the tree contain tests that\n",
      "    examine the features and, based on the outcome, direct you to the appropriate subtree. Given\n",
      "    $\\v{x}$, walking from root to leaves, as directed by the tests, takes you to neighbours in\n",
      "    time that is logarithmic in the number of examples. (It is not necessarily so cheap and easy to find the $k$\n",
      "    nearest neighbours: you might have to 'backup' the tree to visit other subtrees.) $kd$-trees work best,\n",
      "    though, for low dimensional data (small $n$).\n",
      "</p>\n",
      "<p>\n",
      "    Another such data structure is the ball tree, which is based on constructing nested hyper-spheres. Building\n",
      "    a ball tree is more costly than building a $kd$-tree. But ball trees find neighbours quickly, even for high \n",
      "    dimensional data.\n",
      "</p>\n",
      "<p>\n",
      "    By default, scikit-learn will choose the data structure for you. But if you prefer, you can specify it\n",
      "   explicitly using the algorithm parameter, whose values are \u2018auto\u2019 (default), \u2018ball_tree\u2019, \u2018kd_tree\u2019 or \u2018brute\u2019, e.g.:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = KNeighborsRegressor(n_neighbors = 1, algorithm = 'kd_tree')\n",
      "estimator.fit(X, y)\n",
      "\n",
      "# Example prediction\n",
      "estimator.predict([[114, 3, 2]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "array([ 385.])"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    For more details, see the \n",
      "    <a href=\"http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms\">on-line documentation</a>.\n",
      "</p>\n",
      "<p>\n",
      "    Another way to reduce the cost of nearest neighbour methods is to edit the training set, i.e. to delete examples so\n",
      "    that $m$ is smaller. Obviously, the key is in choosing the right examples to delete: ones which do little damage to\n",
      "    prediction accuracy. We will not look at these ideas any further in this module.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The Curse of Dimensionality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Typically, the distance measure is computed across <em>all</em> the features and (other things being equal)\n",
      "    treats them as equally important. But some may be less important than others &mdash; or even irrelevant. \n",
      "    (By contrast, in some sense, methods such as linear regression <em>discover</em> the relative importances of the\n",
      "    features.) Using these features in the distance calculation can have negative effects.\n",
      "</p>\n",
      "<p>\n",
      "    Consider a problem in which there are 30 features but only two are relevant. Two examples might be\n",
      "    identical on the two relevant features. But their distance, when all 30 features are used in the\n",
      "    calculation, might be very high. The neighbours found by such a distance function will then not be ones \n",
      "    that are truly predictive of the dependent variable, resulting in poor predictions.\n",
      "</p>\n",
      "<p>\n",
      "    The more features there are, the more likely it is that situations like this will arise. This is the\n",
      "    so-called curse of dimensionality (where dimensionality refers to $n$, the number of features). For\n",
      "    a time, it was thought that this confined kNN methods to datasets with few features (small $n$), \n",
      "    preferably where all of them are of roughly equal importance.\n",
      "</p>\n",
      "<p>\n",
      "    But there is a solution that can allow kNN to be used more widely: to use feature weights in the distance\n",
      "    function. These weights capture the relative importances of each feature (or even, when zero, they\n",
      "    select which features are irrelevant):\n",
      "    $$d(\\v{x}, \\v{x}') = \\sqrt{\\frac{\\sum_{j=1}^n \\v{w}_j \\times (\\v{x}_j - \\v{x}_j')^2}{\\sum_{j=1}^n \\v{w}_j}}$$\n",
      "    Be clear: these feature weights ($\\v{w}_j$) are quite different from the distance weights ($\\v{w}^{(i)}$)\n",
      "    that we used earlier.\n",
      "</p>\n",
      "<p>\n",
      "    Where will the feature weights come from? It's possible that your domain expert can supply them\n",
      "    up-front. But, in machine learning, we always tend to prefer to use the data to help us to find them.\n",
      "    They are further examples of hyperparameters and so they can be set in the same way as $k$ (see the\n",
      "    up-coming lecture).\n",
      "</p>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}