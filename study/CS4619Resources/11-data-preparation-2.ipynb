{
 "metadata": {
  "name": "",
  "signature": "sha256:879260f9f6f337c6ecc6634e7f83f22a84bc9930f01441c9a99308b46413a9bd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "CS4619: Artificial Intelligence 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Preparation II"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Derek Bridge<br>\n",
      "School of Computer Science and Information Technology<br>\n",
      "University College Cork"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Initialization\n",
      "$\\newcommand{\\Set}[1]{\\{#1\\}}$\n",
      "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$\n",
      "$\\newcommand{\\v}[1]{\\pmb{#1}}$\n",
      "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$\n",
      "$\\newcommand{\\rv}[1]{[#1]}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "We continue our study of <b>data preparation</b> with these topics:\n",
      "<ul>\n",
      "    <li>\n",
      "        Handling nominal-valued features\n",
      "    </li>\n",
      "    <li>\n",
      "        Scaling numeric values\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkB.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Handling Nominal-Valued Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Most learning algorithms work only with numeric-valued features. (There are exceptions.) This is true,\n",
      "    for example, of the OLS linear regression algorithms that we have looked at; it is less true of\n",
      "    kNN, where you can define a special distance function that works on nominal-valued features. \n",
      "</p>\n",
      "<p>\n",
      "    We will look at how to convert nominal-valued features to numeric-valued ones.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Binary-valued features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The simplest case, obviously, is a binary-valued feature. One value is encoded as 0 and the other as 1,\n",
      "    e.g. 'M' is 0 and 'F' is 1. In the example below, 'SecondHand' is 0 and 'New' is 1:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Not only does this replace the values, it also changes the data type too :)\n",
      "df.replace({'devment' : {'SecondHand' : 0, 'New' : 1}}, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Unordered nominal values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Suppose there are more than two values, e.g. Apartment, Detached, SemiDetached or Terraced. The\n",
      "    obvious thing to do is to assign integers to each nominal value, e.g. 0 = Apartment, 1 = Detached, etc. \n",
      "</p>\n",
      "<p>\n",
      "    In pandas, we could do this using the <code>replace</code> function, similar to what we did for the\n",
      "    $\\mathit{devment}$ feature above.\n",
      "</p>\n",
      "<p>\n",
      "    But often this is not the best encoding. And that's certainly true here. Think of the original values\n",
      "    for this feature. It does not make sense to calculate with them: what is Terraced multiplied by \n",
      "    SemiDetached? It does not even make sense to compare them: is Apartment greater than Terraced? But if we \n",
      "    use a simple numeric encoding like the one above a learning algorithm might mistakenly find such numeric\n",
      "    relationships in the data.\n",
      "</p>\n",
      "<p>\n",
      "    Instead, we use <b>one-hot encoding</b>. If the original nominal-valued feature has $p$ values, then\n",
      "    we use $p$ binary-valued features: in each example, exactly one of them is set to 1 and the rest are\n",
      "    zero.\n",
      "</p>\n",
      "<p>\n",
      "    For example, there are four types of dwelling, so we have four binary-valued features. The first is set to 1 \n",
      "    if and only if the type of dwelling is Apartment; the second is set to 1 if and only if the house is \n",
      "    Detached; and so on. So for these four features a detached house will have $\\rv{0, 1, 0, 0}$ as their\n",
      "    values.\n",
      "</p>\n",
      "<p>\n",
      "    Some questions:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        One-hot encoding replaces one nominal-valued feature that has $p$ values by $p$ binary-valued ones &mdash; in\n",
      "        general, one feature per nominal value. (E.g. $\\mathit{type}$ has four values, so we get four binary fetaures.)\n",
      "        What is the minimum number of binary-valued features we could use? \n",
      "    </li>\n",
      "    <li>\n",
      "        Why don't we use the minimum?\n",
      "    </li>\n",
      "    <li>\n",
      "        Although we get $p$ binary features, we only need $p - 1$. How come? (Advanced note: Look up the\n",
      "        <i>dummy variable trap</i> to see why this might even be preferable.)\n",
      "    </li>\n",
      "    <li>\n",
      "        How might one encode a feature whose values are sets of nominal values (such as the movie genre example\n",
      "        given in the previous lecture)?\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    In practice, it is not uncommon to be given a dataset where a nominal-valued feature has already been \n",
      "    encoded numerically, one integer per value. You might be fooled into thinking that the feature is\n",
      "    numeric-valued and overlook the need to use one-hot encoding on it. Watch out for this!\n",
      "</p>\n",
      "<p>\n",
      "    So let's see how to do this in pandas:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The get_dummies() function does the one-hot encoding. Note that NaNs will be replaced by all zeros\n",
      "one_hot = pd.get_dummies(df['type'], 'type', '_')\n",
      "# And the next two statements 'replace' the existing feature by the new binary-valued features\n",
      "# First, drop the existing column\n",
      "df.drop('type', axis=1, inplace=True)\n",
      "# Next, concatenate the new columns. This assumes no clash of column names.\n",
      "df = pd.concat([df, one_hot], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Ordered nominal values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Consider the case now of a feature whose values are nominal but where there <em>is</em> an ordering. The \n",
      "    $\\mathit{ber}$ feature in the housing dataset is like this, for example. In this case, \n",
      "    G &lt; F &lt; E2 &lt; E1 &lt; D1 ... &lt; A1 Some people would use the phrase 'ordinal-valued' to refer \n",
      "    to nominal values that have an ordering.\n",
      "</p>\n",
      "<p>\n",
      "    You might be tempted to use a straightforward numeric encoding, e.g. 0 = G, 1 = F, 2 = E2, 3 = E1,\n",
      "    and so on. This encoding preserves the ordering, e.g. that E2 &lt; E1 because\n",
      "    2 &lt; 3. But again this is probably not the best encoding. First, it has become possible to do\n",
      "    calculations on the new feature that were not possible on the original, e.g. to multiply a F\n",
      "    by a E2 ($1 \\times 2 = 2$ &mdash; but what does that mean?) Second, the original feature had an ordering on\n",
      "    its values but no notion of distance. For example, G &lt; F but you cannot say by \n",
      "    <em> how much</em> G is less than F. In the new feature, we have introduced a notion of distance:\n",
      "    G is worse than F by 1, and it is 2 worse than E2. So this encoding has <em>added</em> \n",
      "    'information' that was not present in the original.\n",
      "</p>\n",
      "<p>\n",
      "    So what should we do?\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        We could use one-hot encoding: fifteen binary-valued features. But what's the weakness of this?\n",
      "    </li>\n",
      "    <li>\n",
      "        Another option is to use binary-valued features that represent inequalities. For example,\n",
      "        one feature is set to 1 if you have a BER of at least G; another is additionally set to 1 if you have\n",
      "        attained at least F, and so on &mdash; still fifteen binary-valued features, but no longer\n",
      "        mutually exclusive. So, for example, a BER of E2 is converted to the following fifteen binary-valued\n",
      "        features: $\\rv{1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}$ and a BER of E1 is converted to\n",
      "        $\\rv{1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}$.\n",
      "        Unfortunately, I don't know of an method in a library for doing this &mdash; we would have to write our\n",
      "        own. So the temptation will be to 'make do' with one-hot encoding.\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    Again watch out for cases where some well-intentioned person has already encoded this kind of feature\n",
      "    but using a naive numeric encoding.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Scaling Numeric Values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Different numeric-valued features often have very different ranges. For example, the values for floor area \n",
      "    are going to range from a few tens to a few hundreds of square metres; but the number of bedrooms and\n",
      "    bathrooms is going to range from 0 to a dozen or so at most. Often this is a problem for a learning algorithm. \n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        OLS linear regression:\n",
      "        <ul>\n",
      "            <li>\n",
      "                The normal equation: \n",
      "                <ul>\n",
      "                    <li>\n",
      "                        Having features with very different ranges of values causes no problem.\n",
      "                    </li>\n",
      "                </ul>\n",
      "            </li>\n",
      "            <li>\n",
      "                Gradient descent: \n",
      "                <ul>\n",
      "                    <li>\n",
      "                    It <em>is</em> a problem in this case. It becomes very difficult to find a value for the \n",
      "                        learning rate $\\alpha$. A 'large' value for $\\alpha$ makes big changes in $\\beta_1$ \n",
      "                        (the floor area parameter), with a risk of overshooting the minimum or even of \n",
      "                        divergence; a 'small' enough value avoids this problem but makes such small changes \n",
      "                        to $\\beta_2$ and $\\beta_3$ (the parameters for the number of bedrooms and bathrooms) \n",
      "                        that convergence may take a very long time.\n",
      "                    </li>\n",
      "                </ul>\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "    <li>Lasso and Ridge Regression:\n",
      "        <ul>\n",
      "            <li>It is generally thought to be useful to scale the features, irrespective of how the learner\n",
      "                is implemented.\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>    \n",
      "    <li>kNN:\n",
      "        <ul>\n",
      "            <li>\n",
      "                Features with large ranges will dominate the distance calculations, thus giving features with\n",
      "                small ranges negligible influence on finding neighbours.\n",
      "            </li>\n",
      "        </ul>\n",
      "    </li>\n",
      "</ul>\n",
      "<p>\n",
      "    The solution is to scale the values so that they have similar ranges. We'll discuss \n",
      "    several ways to do this. (Note that some people call this <b>normalization</b>, but it is not related to\n",
      "    the normalization you may have studied in the context of relational databases.)\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Min-Max Scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Suppose we want to scale feature $j$. Let $max_j$ be the maximum possible value for this feature, which\n",
      "    can be supplied by your domain expert. A quick-and-dirty way to scale the values to $[0,1]$ is to \n",
      "    divide each value $\\v{x}_j$ by $max_j$:\n",
      "    $$\\v{x}_j \\gets \\frac{\\v{x}_j}{max_j}$$\n",
      "    For example, Ann tells us that no house will be above 500 square\n",
      "    metres and no house will have more than 10 bedrooms. So you divide values for the former by 500 and you \n",
      "    divide values for the latter by 10.\n",
      "</p>\n",
      "<p>\n",
      "    Suppose your domain expert also supplies a minimum possible value $min_j$. Then a slightly improved\n",
      "    way to scale to $[0, 1]$ is to subtract the minimum value and divide by the range:\n",
      "    $$\\v{x}_j \\gets \\frac{\\v{x}_j - min_j}{max_j - min_j}$$\n",
      "    For example, Ann tells us that the smallest houses are 40 square metres and the largest are 500 square \n",
      "    metres, so we subtract 40 and divide by $500 - 40 = 460$.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Min-Max Scaling in pandas and scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Min-max scaling in pandas is pretty easy:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bdrms, where min = 1 and max = 10\n",
      "# Subtract the min\n",
      "df['bdrms'] -= 1\n",
      "# Divide by the range, i.e. max - min\n",
      "df['bdrms'] /= 9\n",
      "\n",
      "# A quick check\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>flarea</th>\n",
        "      <th>bdrms</th>\n",
        "      <th>bthrms</th>\n",
        "      <th>floors</th>\n",
        "      <th>devment</th>\n",
        "      <th>price</th>\n",
        "      <th>type_Apartment</th>\n",
        "      <th>type_Detached</th>\n",
        "      <th>type_Semi-detached</th>\n",
        "      <th>type_Terraced</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td>  233.000000</td>\n",
        "      <td> 234.000000</td>\n",
        "      <td> 236.000000</td>\n",
        "      <td> 233.000000</td>\n",
        "      <td> 229.000000</td>\n",
        "      <td>  224.000000</td>\n",
        "      <td> 236.000000</td>\n",
        "      <td> 236.000000</td>\n",
        "      <td> 236.000000</td>\n",
        "      <td> 236.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>  145.458247</td>\n",
        "      <td>   0.276353</td>\n",
        "      <td>   2.161017</td>\n",
        "      <td>   1.841202</td>\n",
        "      <td>   0.013100</td>\n",
        "      <td>  307.714286</td>\n",
        "      <td>   0.118644</td>\n",
        "      <td>   0.326271</td>\n",
        "      <td>   0.296610</td>\n",
        "      <td>   0.258475</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>  134.103000</td>\n",
        "      <td>   0.136270</td>\n",
        "      <td>   1.188386</td>\n",
        "      <td>   0.366275</td>\n",
        "      <td>   0.113954</td>\n",
        "      <td>  374.332421</td>\n",
        "      <td>   0.324057</td>\n",
        "      <td>   0.469844</td>\n",
        "      <td>   0.457734</td>\n",
        "      <td>   0.438727</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>    0.371600</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   55.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>   81.800000</td>\n",
        "      <td>   0.222222</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>  168.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>  109.000000</td>\n",
        "      <td>   0.222222</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>  225.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>  167.200000</td>\n",
        "      <td>   0.333333</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>  345.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 1301.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>  10.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> 3800.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 10 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "            flarea       bdrms      bthrms      floors     devment  \\\n",
        "count   233.000000  234.000000  236.000000  233.000000  229.000000   \n",
        "mean    145.458247    0.276353    2.161017    1.841202    0.013100   \n",
        "std     134.103000    0.136270    1.188386    0.366275    0.113954   \n",
        "min       0.371600    0.000000    1.000000    1.000000    0.000000   \n",
        "25%      81.800000    0.222222    1.000000    2.000000    0.000000   \n",
        "50%     109.000000    0.222222    2.000000    2.000000    0.000000   \n",
        "75%     167.200000    0.333333    3.000000    2.000000    0.000000   \n",
        "max    1301.000000    1.000000   10.000000    2.000000    1.000000   \n",
        "\n",
        "             price  type_Apartment  type_Detached  type_Semi-detached  \\\n",
        "count   224.000000      236.000000     236.000000          236.000000   \n",
        "mean    307.714286        0.118644       0.326271            0.296610   \n",
        "std     374.332421        0.324057       0.469844            0.457734   \n",
        "min      55.000000        0.000000       0.000000            0.000000   \n",
        "25%     168.000000        0.000000       0.000000            0.000000   \n",
        "50%     225.000000        0.000000       0.000000            0.000000   \n",
        "75%     345.000000        0.000000       1.000000            1.000000   \n",
        "max    3800.000000        1.000000       1.000000            1.000000   \n",
        "\n",
        "       type_Terraced  \n",
        "count     236.000000  \n",
        "mean        0.258475  \n",
        "std         0.438727  \n",
        "min         0.000000  \n",
        "25%         0.000000  \n",
        "50%         0.000000  \n",
        "75%         1.000000  \n",
        "max         1.000000  \n",
        "\n",
        "[8 rows x 10 columns]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    scikit-learn provides a class called <code>MinMaxScaler</code>, which does something similar. However:\n",
      "</p>\n",
      "<ul>\n",
      "    <li>\n",
      "        Above, we supplied the min (the smallest <em>possible</em> value) and the max (the largest <em>possible</em>\n",
      "        value) &mdash; presumably we got them from our domain expert.\n",
      "    </li>\n",
      "    <li>\n",
      "        In scikit-learn, the min and max are computed from the data: the smallest and largest <em>actual</em> values\n",
      "        in the dataset.\n",
      "    </li>\n",
      "    <li>\n",
      "        <b>Question:</b> What might potentially go wrong by using scikit-learn's approach?\n",
      "    </li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Demo I: Usefulnesss of Scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We'll show that scaling makes it easier to find a value of $\\alpha$ that results in convergence when doing\n",
      "    OLS linear regression using Gradient Descent Search. To illustrate, let's use the original version of the \n",
      "    Cork property dataset (which has just $\\mathit{flarea}$, $\\mathit{bdrms}$ and $\\mathit{bthrms}$ features). \n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def J(beta, X, y):\n",
      "    \"\"\"\n",
      "    Loss function for OLS regression\n",
      "    \"\"\"\n",
      "    h = X.dot(beta)\n",
      "    differences = h.flatten() - y\n",
      "    sq_differences = differences ** 2\n",
      "    return (1.0 / (2 * y.size)) * sq_differences.sum()\n",
      "\n",
      "def gds_for_ols_linear_regression(X, y, alpha, num_iterations):\n",
      "    \"\"\"\n",
      "    Gradient descent search for OLS linear regression.\n",
      "    alpha is the learning rate.\n",
      "    num_iterations is the numer of updates - instead of a better definition of convergence.\n",
      "    It returns parameters beta and also a numpy array of size num_iterations, containing\n",
      "    the value of the loss function, J, after each iteration - so you can plot it.\n",
      "    \"\"\"\n",
      "\n",
      "    Jvals = np.zeros(num_iterations)\n",
      "    m, n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    \n",
      "    for iter in range(num_iterations):\n",
      "        beta -= (1.0 * alpha / m) * (X.dot(beta) - y).dot(X)\n",
      "        Jvals[iter] = J(beta, X, y)\n",
      " \n",
      "    return beta, Jvals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Before scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Let's try to learn a linear model on the original data (not scaled) with Gradient Descent. See if you can find a \n",
      "    value of $\\alpha$ that makes it work out:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# Insert the extra feature (all ones)\n",
      "df.insert(loc=0, column='ones', value=1)\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['ones', 'flarea', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values\n",
      "\n",
      "# Run the GDS\n",
      "beta, Jvals = gds_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 4000)\n",
      "\n",
      "# Display the final value of J\n",
      "Jvals[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "nan"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Scale the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# flarea, where min = 40 and max = 500\n",
      "# Subtract the min\n",
      "df['flarea'] -= 40\n",
      "# Divide by the range, i.e. max - min\n",
      "df['flarea'] /= 460\n",
      "\n",
      "# bdrms, where min = 0 and max = 10\n",
      "# Subtract the min\n",
      "# df['bdrms'] -= 0\n",
      "# Divide by the range, i.e. max - min\n",
      "df['bdrms'] /= 10\n",
      "\n",
      "# bthrms, where min = 0 and max = 10\n",
      "# Subtract the min\n",
      "# df['bdrms'] -= 0\n",
      "# Divide by the range, i.e. max - min\n",
      "df['bthrms'] /= 10\n",
      "\n",
      "# A quick check\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>flarea</th>\n",
        "      <th>bdrms</th>\n",
        "      <th>bthrms</th>\n",
        "      <th>price</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 224.000000</td>\n",
        "      <td> 224.000000</td>\n",
        "      <td> 224.000000</td>\n",
        "      <td>  224.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>   0.198869</td>\n",
        "      <td>   0.345089</td>\n",
        "      <td>   0.213839</td>\n",
        "      <td>  307.714286</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>   0.177207</td>\n",
        "      <td>   0.121548</td>\n",
        "      <td>   0.118796</td>\n",
        "      <td>  374.332421</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>   0.002174</td>\n",
        "      <td>   0.100000</td>\n",
        "      <td>   0.100000</td>\n",
        "      <td>   55.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>   0.093641</td>\n",
        "      <td>   0.300000</td>\n",
        "      <td>   0.100000</td>\n",
        "      <td>  168.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>   0.147826</td>\n",
        "      <td>   0.300000</td>\n",
        "      <td>   0.200000</td>\n",
        "      <td>  225.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>   0.246685</td>\n",
        "      <td>   0.400000</td>\n",
        "      <td>   0.300000</td>\n",
        "      <td>  345.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>   0.993478</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> 3800.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 4 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "           flarea       bdrms      bthrms        price\n",
        "count  224.000000  224.000000  224.000000   224.000000\n",
        "mean     0.198869    0.345089    0.213839   307.714286\n",
        "std      0.177207    0.121548    0.118796   374.332421\n",
        "min      0.002174    0.100000    0.100000    55.000000\n",
        "25%      0.093641    0.300000    0.100000   168.000000\n",
        "50%      0.147826    0.300000    0.200000   225.000000\n",
        "75%      0.246685    0.400000    0.300000   345.000000\n",
        "max      0.993478    1.000000    1.000000  3800.000000\n",
        "\n",
        "[8 rows x 4 columns]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "After scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    And now it should be much easier to find a value of $\\alpha$ that works:\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Insert the extra feature (all ones)\n",
      "df.insert(loc=0, column='ones', value=1)\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['ones', 'flarea', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values\n",
      "\n",
      "# Run the GDS\n",
      "beta, Jvals = gds_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 4000)\n",
      "\n",
      "# Display the final value of J\n",
      "Jvals[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "30767.643737838611"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Demo II: Usefulness of Scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Now we'll see how useful scaling is for Lasso Regression, Ridge Regression and kNN, again using the original \n",
      "    Cork property dataset.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Before scaling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['flarea', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "\n",
      "knn = KNeighborsRegressor(n_neighbors = 3)\n",
      "mses_test = np.abs(cross_val_score(knn, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "136650.68151075978"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LassoCV\n",
      "lassoCV = LassoCV(cv = 10)\n",
      "mses_test = np.abs(cross_val_score(lassoCV, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "92299.83116105033"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import RidgeCV\n",
      "ridgeCV = RidgeCV(cv = 10)\n",
      "mses_test = np.abs(cross_val_score(ridgeCV, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "99203.60747200335"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Scale the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# flarea, where min = 40 and max = 500\n",
      "# Subtract the min\n",
      "df['flarea'] -= 40\n",
      "# Divide by the range, i.e. max - min\n",
      "df['flarea'] /= 460\n",
      "\n",
      "# bdrms, where min = 0 and max = 10\n",
      "# Subtract the min\n",
      "# df['bdrms'] -= 0\n",
      "# Divide by the range, i.e. max - min\n",
      "df['bdrms'] /= 10\n",
      "\n",
      "# bthrms, where min = 0 and max = 10\n",
      "# Subtract the min\n",
      "# df['bdrms'] -= 0\n",
      "# Divide by the range, i.e. max - min\n",
      "df['bthrms'] /= 10\n",
      "\n",
      "# Get the feature-values and the target values into separate numpy arrays of numbers\n",
      "X = df[['flarea', 'bdrms', 'bthrms']].values\n",
      "y = df['price'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "After scaling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mses_test = np.abs(cross_val_score(knn, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "112618.28019323673"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mses_test = np.abs(cross_val_score(lassoCV, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "93370.253025557817"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mses_test = np.abs(cross_val_score(ridgeCV, X, y, scoring = 'mean_squared_error', cv = 10))\n",
      "mean_mse_test = np.mean(mses_test)\n",
      "mean_mse_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "82830.050380147921"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    We see lower error on the scaled data for kNN and Ridge Regression. \n",
      "    Conventional wisdom and intuition is that we should see the\n",
      "    same in the case of Lasso Regression. But, we don't! I'm not sure why.\n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Standardization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    In some cases, you want feature values not just to have the same range but to have the same mean\n",
      "    and even the same variance.\n",
      "</p>\n",
      "<p>\n",
      "    One idea is <b>mean centering</b>, where you subtract the mean value of the feature. If you do this to all\n",
      "    values, some of the new values will be positive and some will be negative and their mean will be approximately\n",
      "    zero. But the features may still have different ranges. So you might still divide by the maximum or the range.\n",
      "</p>\n",
      "<p>\n",
      "    But better still is <b>standardization</b>, in which you subtract the mean and divide by the standard\n",
      "    deviation:\n",
      "    $$\\v{x}_j \\gets \\frac{\\v{x}_j - \\mu_j}{\\sigma_j}$$\n",
      "    where $\\mu_j$ is the mean of the values for feature $j$ and $\\sigma_j$ is their standard deviation.\n",
      "</p>\n",
      "<p>\n",
      "    If you use this, then the mean will be approximately zero and all values will be expressed in terms of\n",
      "    how many standard deviations they are from the mean. \n",
      "</p>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Standardize the Wrong Way"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    Here it is done incorrectly on the $\\mathit{flarea}$ feature!\n",
      "</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use pandas to read the CSV file\n",
      "df = pd.read_csv(\"dataset-corkA.csv\")\n",
      "\n",
      "# flarea\n",
      "flarea_mean = df['flarea'].mean()\n",
      "flarea_std = df['flarea'].std()\n",
      "df['flarea'] -= flarea_mean\n",
      "df['flarea'] /= flarea_std\n",
      "\n",
      "# A quick check\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>flarea</th>\n",
        "      <th>bdrms</th>\n",
        "      <th>bthrms</th>\n",
        "      <th>price</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 2.240000e+02</td>\n",
        "      <td> 224.000000</td>\n",
        "      <td> 224.000000</td>\n",
        "      <td>  224.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>-8.103637e-17</td>\n",
        "      <td>   3.450893</td>\n",
        "      <td>   2.138393</td>\n",
        "      <td>  307.714286</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>   1.215483</td>\n",
        "      <td>   1.187962</td>\n",
        "      <td>  374.332421</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-1.109974e+00</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   55.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-5.938135e-01</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>  168.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>-2.880428e-01</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>  225.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 2.698276e-01</td>\n",
        "      <td>   4.000000</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>  345.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 4.484065e+00</td>\n",
        "      <td>  10.000000</td>\n",
        "      <td>  10.000000</td>\n",
        "      <td> 3800.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 4 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "             flarea       bdrms      bthrms        price\n",
        "count  2.240000e+02  224.000000  224.000000   224.000000\n",
        "mean  -8.103637e-17    3.450893    2.138393   307.714286\n",
        "std    1.000000e+00    1.215483    1.187962   374.332421\n",
        "min   -1.109974e+00    1.000000    1.000000    55.000000\n",
        "25%   -5.938135e-01    3.000000    1.000000   168.000000\n",
        "50%   -2.880428e-01    3.000000    2.000000   225.000000\n",
        "75%    2.698276e-01    4.000000    3.000000   345.000000\n",
        "max    4.484065e+00   10.000000   10.000000  3800.000000\n",
        "\n",
        "[8 rows x 4 columns]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Standardization the Correct Way"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>\n",
      "    The problem with the approach above is the same problem that we discussed when imputing means for missing values:\n",
      "    <b>leakage</b>.\n",
      "</p>\n",
      "<p>\n",
      "    The solution is to use scikit-learn's <code>StandardScaler</code> and <code>Pipeline</code> classes.\n",
      "</p>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}