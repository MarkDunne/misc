Linear Regression

	x superscript i = ith example of x
	x subscript j = jth feature in x


KNN
	Uniform, Distance, Shepards

	Advantages
		Can fit complex data
		Can be easily modified to use non-numeric data given the correct distance formula
		Simple to incrementally add more data
	Disadvantages
		High Prediction cost
		Curse of dimensionality
			Naive KNN cannot evaluate the importance of features, which can lead to problem.
			But KNN can use feature weights in the distance formula. 
			Weights can come from domain expert or as hyperparameters

Error Estimation
	Holdout
		High variance in results
	Repeated Holdout
		Higher confidence than holdout, but training/test sets may overlap
	k-Fold
		High variance between folds, but no overlap
		Can't allow any fold with less than 30 examples
	Repeated k-Fold
		High confidence, no overlap
	Leave-One-Out
		Max amount of training data used
		No randomness between results (good)
		But, expensive (not so much for kNN where training is cheap)

Model Selection


Underfitting / Overfitting
	Underfitting - model not complex enough
	Overfitting  - model too complex

	Know shape of bias/variance curve (test and training lines)


	Learning Curve
		The test score tends towards the irreducible error
		
Classification Performance
	ROC Space
		bottom­left to top­right
			TPR = FPR 
			along the line is essentially random
			above line - better than random
			below line - worse than random
		top-left to bottom-right
			TPR = 1 - FPR
			along the line classifier performs equally well on positives and negatives
			above of line - better on positive examples
			below of line - better on negative examples

	Precision - Out of all the examples that were thought to be positive, how many actually were postitive
	Recall - Out of all the examples that were actually positive, how many were thought to be positive
